{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e965f5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-18T14:19:34.733622Z",
     "iopub.status.busy": "2024-11-18T14:19:34.733288Z",
     "iopub.status.idle": "2024-11-18T14:20:22.775693Z",
     "shell.execute_reply": "2024-11-18T14:20:22.774712Z"
    },
    "papermill": {
     "duration": 48.05178,
     "end_time": "2024-11-18T14:20:22.778175",
     "exception": false,
     "start_time": "2024-11-18T14:19:34.726395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\r\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tiktoken\r\n",
      "Successfully installed tiktoken-0.8.0\r\n",
      "Collecting triton\r\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton) (3.15.1)\r\n",
      "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton\r\n",
      "Successfully installed triton-3.1.0\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n",
      "W1118 14:20:21.739000 138775679018816 torch/distributed/run.py:779] \r\n",
      "W1118 14:20:21.739000 138775679018816 torch/distributed/run.py:779] *****************************************\r\n",
      "W1118 14:20:21.739000 138775679018816 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W1118 14:20:21.739000 138775679018816 torch/distributed/run.py:779] *****************************************\r\n",
      "/opt/conda/bin/python3.10: can't open file '/kaggle/input/model33/pytorch/default/1/GPT_2_build.py': [Errno 2] No such file or directory\r\n",
      "/opt/conda/bin/python3.10: can't open file '/kaggle/input/model33/pytorch/default/1/GPT_2_build.py': [Errno 2] No such file or directory\r\n",
      "E1118 14:20:21.990000 138775679018816 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 2) local_rank: 0 (pid: 67) of binary: /opt/conda/bin/python3.10\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/torchrun\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\r\n",
      "    return f(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\r\n",
      "    run(args)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\r\n",
      "    elastic_launch(\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 133, in __call__\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\r\n",
      "    raise ChildFailedError(\r\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n",
      "============================================================\r\n",
      "/kaggle/input/model33/pytorch/default/1/GPT_2_build.py FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "[1]:\r\n",
      "  time      : 2024-11-18_14:20:21\r\n",
      "  host      : 5be35f62e661\r\n",
      "  rank      : 1 (local_rank: 1)\r\n",
      "  exitcode  : 2 (pid: 68)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2024-11-18_14:20:21\r\n",
      "  host      : 5be35f62e661\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : 2 (pid: 67)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "!pip install triton\n",
    "!pip install datasets\n",
    "!torchrun --standalone --nproc_per_node=2 /kaggle/input/model33/pytorch/default/1/GPT_2_build.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3db4b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T14:20:22.802697Z",
     "iopub.status.busy": "2024-11-18T14:20:22.802316Z",
     "iopub.status.idle": "2024-11-18T14:20:22.813859Z",
     "shell.execute_reply": "2024-11-18T14:20:22.813075Z"
    },
    "papermill": {
     "duration": 0.025706,
     "end_time": "2024-11-18T14:20:22.815706",
     "exception": false,
     "start_time": "2024-11-18T14:20:22.790000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Downloads and evaluates HellaSwag in Python.\n",
    "# https://github.com/rowanz/hellaswag\n",
    "\n",
    "# Example HellaSwag json item:\n",
    "\n",
    "# {\"ind\": 24, \"activity_label\": \"Roof shingle removal\", \"ctx_a\": \"A man is sitting on a roof.\", \"ctx_b\": \"he\", \"ctx\": \"A man is sitting on a roof. he\", \"split\": \"val\", \"split_type\": \"indomain\", \"label\": 3, \"endings\": [\"is using wrap to wrap a pair of skis.\", \"is ripping level tiles off.\", \"is holding a rubik's cube.\", \"starts pulling up roofing on a roof.\"], \"source_id\": \"activitynet~v_-JhWjGDPHMY\"}\n",
    "\n",
    "# ind: dataset ID\n",
    "# activity_label: The ActivityNet or WikiHow label for this example\n",
    "# context: There are two formats. The full context is in ctx. When the context ends in an (incomplete) noun phrase, like for ActivityNet, this incomplete noun phrase is in ctx_b, and the context up until then is in ctx_a. This can be useful for models such as BERT that need the last sentence to be complete. However, it's never required. If ctx_b is nonempty, then ctx is the same thing as ctx_a, followed by a space, then ctx_b.\n",
    "# endings: a list of 4 endings. The correct index is given by label (0,1,2, or 3)\n",
    "# split: train, val, or test.\n",
    "# split_type: indomain if the activity label is seen during training, else zeroshot\n",
    "# source_id: Which video or WikiHow article this example came from\n",
    "\n",
    "# gpt2 (124M)\n",
    "# - eleuther harness reports acc 28.92%, acc_norm 31.14% (multiple choice style)\n",
    "# - this script: 10042 acc: 0.2859 acc_norm: 0.2955 (completion style)\n",
    "\n",
    "# gpt2-xl (1558M)\n",
    "# - eleuther harness reports acc 40.04%, acc_norm 50.89% (multiple choice style)\n",
    "# - this script: 10042 acc: 0.3842 acc_norm: 0.4893 (completion style)\n",
    "\n",
    "# The validation set of HellaSwag has a total of 10,042 examples.\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import requests\n",
    "# import tiktoken\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "# from transformers import GPT2LMHeadModel\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), \"hellaswag\")\n",
    "\n",
    "# def download_file(url: str, fname: str, chunk_size=1024):\n",
    "#     \"\"\"Helper function to download a file from a given url\"\"\"\n",
    "#     resp = requests.get(url, stream=True)\n",
    "#     total = int(resp.headers.get(\"content-length\", 0))\n",
    "#     with open(fname, \"wb\") as file, tqdm(\n",
    "#         desc=fname,\n",
    "#         total=total,\n",
    "#         unit=\"iB\",\n",
    "#         unit_scale=True,\n",
    "#         unit_divisor=1024,\n",
    "#     ) as bar:\n",
    "#         for data in resp.iter_content(chunk_size=chunk_size):\n",
    "#             size = file.write(data)\n",
    "#             bar.update(size)\n",
    "\n",
    "# hellaswags = {\n",
    "#     \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
    "#     \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
    "#     \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
    "# }\n",
    "\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# def download(split):\n",
    "#     \"\"\"Downloads HellaSwag DATA_CACHE_DIR\"\"\"\n",
    "#     os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "#     data_url = hellaswags[split]\n",
    "#     data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
    "#     if not os.path.exists(data_filename):\n",
    "#         print(f\"Downloading {data_url} to {data_filename}...\")\n",
    "#         download_file(data_url, data_filename)\n",
    "\n",
    "# def render_example(example):\n",
    "#     \"\"\"\n",
    "#     Given the example as a dictionary, render it as three torch tensors:\n",
    "#     - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)\n",
    "#     - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)\n",
    "#     - label (the index of the correct completion, which we hope has the highest likelihood)\n",
    "#     \"\"\"\n",
    "#     ctx = example[\"ctx\"]\n",
    "#     label = example[\"label\"]\n",
    "#     endings = example[\"endings\"]\n",
    "\n",
    "#     # data needed to reproduce this eval on the C size\n",
    "#     data = {\n",
    "#         \"label\": label,\n",
    "#         \"ctx_tokens\": None,\n",
    "#         \"ending_tokens\": [],\n",
    "#     }\n",
    "\n",
    "#     # gather up all the tokens\n",
    "#     ctx_tokens = enc.encode(ctx)\n",
    "#     data[\"ctx_tokens\"] = ctx_tokens\n",
    "#     tok_rows = []\n",
    "#     mask_rows = []\n",
    "#     for end in endings:\n",
    "#         end_tokens = enc.encode(\" \" + end) # note: prepending \" \" because GPT-2 tokenizer\n",
    "#         tok_rows.append(ctx_tokens + end_tokens)\n",
    "#         mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
    "#         data[\"ending_tokens\"].append(end_tokens)\n",
    "\n",
    "#     # have to be careful during the collation because the number of tokens in each row can differ\n",
    "#     max_len = max(len(row) for row in tok_rows)\n",
    "#     tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
    "#     mask = torch.zeros((4, max_len), dtype=torch.long)\n",
    "#     for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
    "#         tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
    "#         mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
    "\n",
    "#     return data, tokens, mask, label\n",
    "\n",
    "# def iterate_examples(split):\n",
    "#     # there are 10,042 examples in total in val\n",
    "#     download(split)\n",
    "#     with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
    "#         for line in f:\n",
    "#             example = json.loads(line)\n",
    "#             yield example\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate(model_type, device):\n",
    "\n",
    "#     torch.set_float32_matmul_precision('high') # use tf32\n",
    "#     model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "#     model.to(device)\n",
    "#     # model = torch.compile(model) # optionally torch compile the model\n",
    "\n",
    "#     num_correct_norm = 0\n",
    "#     num_correct = 0\n",
    "#     num_total = 0\n",
    "#     for example in iterate_examples(\"val\"):\n",
    "#         data, tokens, mask, label = render_example(example)\n",
    "#         tokens = tokens.to(device)\n",
    "#         mask = mask.to(device)\n",
    "\n",
    "#         # get the logits\n",
    "#         logits = model(tokens).logits\n",
    "#         # evaluate the autoregressive loss at all positions\n",
    "#         shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "#         shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "#         flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "#         flat_shift_tokens = shift_tokens.view(-1)\n",
    "#         shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "#         shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "#         # now get the average loss just for the completion region (where mask == 1), in each row\n",
    "#         shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n",
    "#         masked_shift_losses = shift_losses * shift_mask\n",
    "#         # sum and divide by the number of 1s in the mask\n",
    "#         sum_loss = masked_shift_losses.sum(dim=1)\n",
    "#         avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "#         # now we have a loss for each of the 4 completions\n",
    "#         # the one with the lowest loss should be the most likely\n",
    "#         pred = sum_loss.argmin().item()\n",
    "#         pred_norm = avg_loss.argmin().item()\n",
    "\n",
    "#         # accumulate stats\n",
    "#         num_total += 1\n",
    "#         num_correct += int(pred == label)\n",
    "#         num_correct_norm += int(pred_norm == label)\n",
    "#         print(f\"{num_total} acc_norm: {num_correct_norm}/{num_total}={num_correct_norm/num_total:.4f}\")\n",
    "\n",
    "#         # debug: pretty print a few examples, and the losses in each case\n",
    "#         if num_total < 10:\n",
    "#             print(\"---\")\n",
    "#             print(f\"Context:\\n {example['ctx']}\")\n",
    "#             print(f\"Endings:\")\n",
    "#             for i, end in enumerate(example[\"endings\"]):\n",
    "#                 print(f\"{i} (loss: {avg_loss[i].item():.4f}) {end}\")\n",
    "#             print(f\"predicted: {pred_norm}, actual: {label}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"-m\", \"--model_type\", type=str, default=\"gpt2\", help=\"the model type to use\")\n",
    "#     parser.add_argument(\"-d\", \"--device\", type=str, default=\"cuda\", help=\"the device to use\")\n",
    "#     args = parser.parse_args()\n",
    "#     evaluate(args.model_type, args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f32060",
   "metadata": {
    "papermill": {
     "duration": 0.011656,
     "end_time": "2024-11-18T14:20:22.838061",
     "exception": false,
     "start_time": "2024-11-18T14:20:22.826405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa20ce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T14:20:22.861527Z",
     "iopub.status.busy": "2024-11-18T14:20:22.861218Z",
     "iopub.status.idle": "2024-11-18T14:20:22.886176Z",
     "shell.execute_reply": "2024-11-18T14:20:22.885497Z"
    },
    "papermill": {
     "duration": 0.038973,
     "end_time": "2024-11-18T14:20:22.888025",
     "exception": false,
     "start_time": "2024-11-18T14:20:22.849052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # scan your code and find the ugly number , we want to get the GPU freindly code\n",
    "\n",
    "# from dataclasses import dataclass\n",
    "# import inspect\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# # prefix tokens\n",
    "# import tiktoken\n",
    "# import os\n",
    "# import json\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# # # Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "# # # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Use GPU 0\n",
    "# # print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "# # # import os\n",
    "# # # import torch\n",
    "\n",
    "# # # Clear CUDA environment setting\n",
    "# # # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "# # # Reinitialize CUDA to clear prior states\n",
    "# # if torch.cuda.is_available():\n",
    "# #     torch.cuda.empty_cache()\n",
    "\n",
    "# # # Attempt to do the autodetect of the device(GPU or CPU)\n",
    "\n",
    "# # device = \"cpu\"\n",
    "# # torch.manual_seed(1337)\n",
    "# # if torch.cuda.is_available():\n",
    "# #     device = \"cuda\"\n",
    "# #     torch.cuda.manual_seed(1337)\n",
    "\n",
    "# # elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "# #     device = \"mps\"\n",
    "\n",
    "# # print(f\"using device : {device}\")\n",
    "\n",
    "\n",
    "# # print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# class CausalSelfAttention(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         assert config.n_embd % config.n_head == 0\n",
    "\n",
    "#         self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "#         self.n_head = config.n_head\n",
    "#         self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "#         self.n_embd = config.n_embd\n",
    "#         self.c_proj.GPT_SCALE = 1\n",
    "#         self.register_buffer(\n",
    "#             \"bias\",\n",
    "#             torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "#                 1, 1, config.block_size, config.block_size\n",
    "#             ),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, T, C = (\n",
    "#             x.size()\n",
    "#         )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "#         # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "#         # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "#         # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "#         qkv = self.c_attn(x)\n",
    "#         q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "#         k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # flash attention\n",
    "#         y = (\n",
    "#             y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "#         )  # re-assemble all head outputs side by side\n",
    "\n",
    "#         # output projection\n",
    "#         y = self.c_proj(y)\n",
    "#         return y\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "#         self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "#         self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "#         self.c_proj.GPT_SCALE = 1\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.c_fc(x)\n",
    "#         x = self.gelu(x)\n",
    "#         x = self.c_proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "#         self.attn = CausalSelfAttention(config)\n",
    "#         self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "#         self.mlp = MLP(config)\n",
    "\n",
    "#         # attention is a communication -  aggregation function , a weigtes sum function a redcue function\n",
    "#         # MLP is a individual function  - MAP function ( MAP reduce )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.ln_1(x))  # self attention layer\n",
    "#         x = x + self.mlp(self.ln_2(x))  # feed forward network\n",
    "#         return x\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class GPTConfig:\n",
    "#     block_size: int = 1024\n",
    "#     vocab_size: int = (\n",
    "#         50304  # number of tokens : 50k + special tokens , overwrite the 50257 with this\n",
    "#     )\n",
    "#     n_layers: int = 12\n",
    "#     n_head: int = 16\n",
    "#     n_embd: int = 1024\n",
    "\n",
    "\n",
    "# class GPT(nn.Module):\n",
    "#     \"\"\"\n",
    "#     GPT (Generative Pre-trained Transformer) model class.\n",
    "\n",
    "#     Args:\n",
    "#         config (GPTConfig): Configuration object containing model hyperparameters.\n",
    "\n",
    "#     Attributes:\n",
    "#         config (GPTConfig): Configuration object containing model hyperparameters.\n",
    "#         transformer (nn.ModuleDict): Dictionary containing the transformer components:\n",
    "#             - wte (nn.Embedding): Token embedding layer.\n",
    "#             - wpe (nn.Embedding): Position embedding layer.\n",
    "#             - h (nn.ModuleList): List of transformer blocks.\n",
    "#             - ln_f (nn.LayerNorm): Final layer normalization.\n",
    "#         lm_head (nn.Linear): Linear layer for output logits.\n",
    "\n",
    "#     Methods:\n",
    "#         forward(idx):\n",
    "#             Forward pass of the GPT model.\n",
    "#             Args:\n",
    "#                 idx (torch.Tensor): Input tensor of shape (B, T) where B is the batch size and T is the sequence length.\n",
    "#             Returns:\n",
    "#                 torch.Tensor: Output logits of shape (B, T, vocab_size).\n",
    "\n",
    "#         from_pretrained(cls, model_type):\n",
    "#             Load a pre-trained GPT model from Hugging Face.\n",
    "#             Args:\n",
    "#                 model_type (str): Type of the pre-trained model to load. Must be one of {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}.\n",
    "#             Returns:\n",
    "#                 GPT: An instance of the GPT model with pre-trained weights.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "\n",
    "#         self.transformer = nn.ModuleDict(\n",
    "#             dict(\n",
    "#                 wte=nn.Embedding(config.vocab_size, config.n_embd),  # token embedding\n",
    "#                 wpe=nn.Embedding(\n",
    "#                     config.block_size, config.n_embd\n",
    "#                 ),  # position embedding\n",
    "#                 h=nn.ModuleList(\n",
    "#                     [Block(config) for _ in range(config.n_layers)]\n",
    "#                 ),  # transformer block\n",
    "#                 ln_f=nn.LayerNorm(config.n_embd),  # final layer normalization\n",
    "#             )\n",
    "#         )\n",
    "#         self.lm_head = nn.Linear(\n",
    "#             config.n_embd, config.vocab_size, bias=False\n",
    "#         )  # head for output\n",
    "#         # weight sharing scheme\n",
    "#         self.transformer.wte.weight = self.lm_head.weight\n",
    "#         self.apply(__init_weight)\n",
    "\n",
    "#     def _init_weight(self, module):\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             std = 0.02\n",
    "#             if hasattr(module, \"GPT_SCALE\"):\n",
    "#                 std *= (2 * self.config.n_layer) ** -0.5\n",
    "#             torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "#             if module.bias is not None:\n",
    "#                 torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "#             elif isinstance(module, nn.Embedding):\n",
    "#                 torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "#     def forward(self, idx, targets=None):\n",
    "#         # idx is of the size ( B , T )\n",
    "#         B, T = idx.size()\n",
    "#         assert (\n",
    "#             T <= self.config.block_size\n",
    "#         ), f\"cannot forward the sequence length {T} , blocksize is less than the {T}\"\n",
    "\n",
    "#         pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "#         pos_emb = self.transformer.wpe(pos)\n",
    "#         tok_emb = self.transformer.wte(idx)\n",
    "#         x = (\n",
    "#             tok_emb + pos_emb\n",
    "#         )  # broadcasting is happening here as the positional emb will be same for each of the samples in the bacth\n",
    "#         for block in self.transformer.h:\n",
    "#             x = block(x)\n",
    "#         x = self.transformer.ln_f(x)\n",
    "#         logits = self.lm_head(x)  # (B , T , vocab_size)\n",
    "#         loss = None\n",
    "#         if targets is not None:\n",
    "#             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "#         return logits, loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, model_type):\n",
    "#         assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "#         from transformers import GPT2LMHeadModel\n",
    "\n",
    "#         print(\"Loading model from Hugging Face: %s\" % model_type)\n",
    "\n",
    "#         config_args = {\n",
    "#             \"gpt2\": dict(\n",
    "#                 n_layers=12,\n",
    "#                 n_embd=768,\n",
    "#                 n_head=12,\n",
    "#             ),\n",
    "#             \"gpt2-medium\": dict(\n",
    "#                 n_layers=24,\n",
    "#                 n_embd=1024,\n",
    "#                 n_head=16,\n",
    "#             ),\n",
    "#             \"gpt2-large\": dict(\n",
    "#                 n_layers=36,\n",
    "#                 n_embd=1280,\n",
    "#                 n_head=20,\n",
    "#             ),\n",
    "#             \"gpt2-xl\": dict(\n",
    "#                 n_layers=48,\n",
    "#                 n_embd=1600,\n",
    "#                 n_head=25,\n",
    "#             ),\n",
    "#         }[model_type]\n",
    "\n",
    "#         config_args[\"vocab_size\"] = 50257\n",
    "#         config_args[\"block_size\"] = 1024\n",
    "#         config = GPTConfig(**config_args)\n",
    "\n",
    "#         model = GPT(config)\n",
    "#         sd = model.state_dict()\n",
    "#         sd_keys = sd.keys()\n",
    "#         sd_keys = [k for k in sd_keys if not k.endswith(\"attn.bias\")]\n",
    "#         model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "#         sd_hf = model_hf.state_dict()\n",
    "#         sd_keys_hf = sd_hf.keys()\n",
    "#         sd_keys_hf = [\n",
    "#             k for k in sd_keys_hf if not k.endswith(\"attn.masked_bias\")\n",
    "#         ]  # remove the bias for attention\n",
    "#         sd_keys_hf = [\n",
    "#             k for k in sd_keys_hf if not k.endswith(\"attn.bias\")\n",
    "#         ]  # remove the bias for attention\n",
    "#         transposed = [\n",
    "#             \"attn.c_attn.weight\",\n",
    "#             \"attn.c_proj.weight\",\n",
    "#             \"mlp.c_fc.weight\",\n",
    "#             \"mlp.c_proj.weight\",\n",
    "#         ]\n",
    "#         print(len(sd_keys_hf), len(sd_keys))\n",
    "#         assert len(sd_keys_hf) == len(\n",
    "#             sd_keys\n",
    "#         ), f\"mismatched keys : {len(sd_keys_hf)  != {len(sd_keys)}}\"\n",
    "\n",
    "#         for k in sd_keys_hf:\n",
    "#             if any(k.endswith(w) for w in transposed):\n",
    "#                 # special treatment for te conv1D weights we need to transpose the matrices\n",
    "#                 assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "#                 with torch.no_grad():\n",
    "#                     sd[k].copy_(sd_hf[k].t())\n",
    "#             else:\n",
    "#                 # vanilla copy over the other parameters\n",
    "#                 assert sd_hf[k].shape == sd[k].shape\n",
    "#                 with torch.no_grad():\n",
    "#                     sd[k].copy_(sd_hf[k])\n",
    "#         return model\n",
    "    \n",
    "#     def configure_optimizer(self , weight_decay, learning_rate, epsilon):\n",
    "#         # Create the optimizer\n",
    "#         param_dict = {pm : p for pm , p in self.named_parameters() } \n",
    "#         param_dict = {k: v for k, v in param_dict.items() if v.requires_grad}\n",
    "#         decay_param = [v for k, v in param_dict.items() if v.dim() >= 2]\n",
    "#         nodecay_param = [v for k, v in param_dict.items() if v.dim()< 2 ]\n",
    "#         optim_groups = [\n",
    "#             {'params': decay_param, 'weight_decay': weight_decay},\n",
    "#             {'params': nodecay_param, 'weight_decay': 0.0}\n",
    "#         ]\n",
    "#         num_decay_params =  sum(p.numel() for p in decay_param)\n",
    "#         num_nodecay_params = sum(p.numel() for p in nodecay_param)\n",
    "#         print(f\"num decayed parameters tensor : {len(decay_param)} , with {num_decay_params} parametere \" )\n",
    "#         print(f\"num non decayed parameters tensor : {len(nodecay_param)} , with {num_nodecay_params} parametere \" )\n",
    "#         fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "#         use_fused = fused_available and 'cuda' in device\n",
    "#         optimizer = torch.optim.AdamW(\n",
    "#             optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=epsilon ,fused=use_fused\n",
    "#         )\n",
    "#         return optimizer\n",
    "\n",
    "\n",
    "# from torch.distributed import init_process_group , destroy_process_group\n",
    "\n",
    "# # ddp = int(os.environ.get(\"RANK\", -1)) != 1 \n",
    "# ddp = None\n",
    "\n",
    "# if ddp : \n",
    "#     assert torch.cuda.is_available()\n",
    "#     init_process_group(backend=\"nccl\")\n",
    "#     ddp_rank = int(os.environ[\"RANK\"])\n",
    "#     ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])  \n",
    "#     device = f\"cuda:{ddp_local_rank}\"\n",
    "#     torch.cuda.set_device(device)\n",
    "#     master_process = ddp_rank == 0\n",
    "# else:\n",
    "#     ddp_rank = 0\n",
    "#     ddp_local_rank = 0  \n",
    "#     ddp_world_size = 1  \n",
    "#     master_process = True\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     print(f\"using device : {device}\")\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "#     torch.cuda.manual_seed(1337)\n",
    "\n",
    "# import time\n",
    "\n",
    "# model = GPT(GPTConfig())\n",
    "\n",
    "# optimizer = model.configure_optimizer(weight_decay=0.1, learning_rate=6e-4, epsilon=1e-8)\n",
    "# # model = torch.nn.DataParallel(model, device_ids=[0, 1]).to(device)\n",
    "# model.to(device)\n",
    "# model = torch.compile(model)\n",
    "# # optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import IterableDataset, DataLoader\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# import queue\n",
    "# import threading\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# class PrefetchingDatasetLoader(IterableDataset):\n",
    "#     def __init__(self, file_path, tokenizer, block_size, chunk_size=1024 * 1024):\n",
    "#         self.file_path = file_path\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.block_size = block_size\n",
    "#         self.chunk_size = chunk_size\n",
    "\n",
    "#     def _tokenize_chunk(self, chunk):\n",
    "#         tokens = self.tokenizer.encode(chunk).ids\n",
    "#         return tokens\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         buffer = \"\"\n",
    "#         tokens = []\n",
    "#         chunk_queue = queue.Queue(maxsize=2)  # Set a small buffer for prefetching\n",
    "\n",
    "#         # Start a background thread to read and tokenize chunks\n",
    "#         def background_loader():\n",
    "#             with open(self.file_path, \"r\") as f:\n",
    "#                 while True:\n",
    "#                     chunk = f.read(self.chunk_size)\n",
    "#                     if not chunk:\n",
    "#                         break\n",
    "#                     # Tokenize and enqueue the chunk\n",
    "#                     tokenized_chunk = self._tokenize_chunk(chunk)\n",
    "#                     chunk_queue.put(tokenized_chunk)\n",
    "\n",
    "#             chunk_queue.put(None)  # Signal that loading is done\n",
    "\n",
    "#         threading.Thread(target=background_loader, daemon=True).start()\n",
    "\n",
    "#         while True:\n",
    "#             # Get the next tokenized chunk\n",
    "#             tokenized_chunk = chunk_queue.get()\n",
    "#             if tokenized_chunk is None:\n",
    "#                 break  # End of file\n",
    "\n",
    "#             tokens.extend(tokenized_chunk)\n",
    "\n",
    "#             # Yield batches from tokens\n",
    "#             while len(tokens) > self.block_size:\n",
    "#                 x = torch.tensor(tokens[: self.block_size], dtype=torch.long)\n",
    "#                 y = torch.tensor(tokens[1 : self.block_size + 1], dtype=torch.long)\n",
    "#                 yield x, y\n",
    "#                 tokens = tokens[self.block_size :]\n",
    "\n",
    "#         # Yield any remaining tokens\n",
    "#         if len(tokens) > self.block_size:\n",
    "#             x = torch.tensor(tokens[: self.block_size], dtype=torch.long)\n",
    "#             y = torch.tensor(tokens[1 : self.block_size + 1], dtype=torch.long)\n",
    "#             yield x, y\n",
    "\n",
    "        \n",
    "# def next_batch(dataloader_iterator):\n",
    "#     \"\"\"\n",
    "#     Fetch the next batch from the dataloader iterator.\n",
    "#     Args:\n",
    "#         dataloader_iterator: Iterator of the DataLoader.\n",
    "#     Returns:\n",
    "#         Tuple (x, y) containing the input and target tensors for the batch.\n",
    "#         Returns None if the iterator is exhausted.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return next(dataloader_iterator)\n",
    "#     except StopIteration:\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "# # Configuration\n",
    "# B, T = 8, 1024  # micro Batch size and token sequence length\n",
    "\n",
    "# total_batch_size = 524288\n",
    "# assert total_batch_size % (B*T*ddp_world_size) == 0\n",
    "# grad_accum_steps = total_batch_size//(B*T*ddp_world_size)\n",
    "# if master_process:  \n",
    "#     print(f\"total desired batch size : {total_batch_size} , grad_accum_steps : {grad_accum_steps}\")\n",
    "\n",
    "# print(\"I am GPU \", ddp_rank)\n",
    "# tokenizer = ByteLevelBPETokenizer(\n",
    "#     \"/kaggle/input/tokens-bytelevel/token_AAA/vocab.json\",\n",
    "#     \"/kaggle/input/tokens-bytelevel/token_AAA/merges.txt\",\n",
    "# )\n",
    "\n",
    "# # Create the dataset and dataloader\n",
    "# dataset = PrefetchingDatasetLoader(\n",
    "#     \"/kaggle/input/python-code-regex/python_code.txt\", tokenizer, block_size=T\n",
    "# )\n",
    "# dataloader = DataLoader(\n",
    "#     dataset, batch_size=B, num_workers=4\n",
    "# )  # Set `num_workers` for parallel loading\n",
    "\n",
    "# # Training loop\n",
    "# # Iterate through batches\n",
    "# # for batch in dataloader:\n",
    "# #     x, y = batch\n",
    "# #     # Your training code here\n",
    "# # scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# max_lr = 6e-4\n",
    "# min_lr = max_lr / 10\n",
    "# warmup_steps = 20 \n",
    "# max_steps = 100\n",
    "# def get_lr(it):\n",
    "#     if it< warmup_steps:\n",
    "#         return max_lr * (it+1) / warmup_steps\n",
    "#     if it > max_steps:\n",
    "#         return min_lr\n",
    "#     decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "#     assert 0.0 <= decay_ratio <= 1.0\n",
    "#     coeff = 0.5 * (1.0 + torch.cos(torch.tensor(decay_ratio) * 3.14159))    \n",
    "#     return min_lr + 0.5 * (max_lr - min_lr) * coeff\n",
    "\n",
    "# use_amp = True\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "# loader = iter(dataloader)\n",
    "# epoch_size = 0\n",
    "# for epoch in range(2):  # Number of epochs\n",
    "#     clips = epoch*max_steps\n",
    "#     # for i, (x, y) in enumerate(dataloader):\n",
    "#     for i in range(50):\n",
    "#         optimizer.zero_grad()\n",
    "#         t0 = time.time()\n",
    "#         loss_accum = 0.0\n",
    "#         for micro_step in range(grad_accum_steps):\n",
    "#             x , y =  next_batch(loader)\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "#             # print(x)\n",
    "#             # print(y)\n",
    "#             # print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "#             #         optimizer.zero_grad()\n",
    "\n",
    "#             # Use autocast for mixed precision\n",
    "#             with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "#                 logits, loss = model(x, y)\n",
    "                \n",
    "#             loss = loss / grad_accum_steps\n",
    "#             loss_accum += loss.detach()\n",
    "#             #         loss.mean().backward()\n",
    "#             #         optimizer.step()\n",
    "#             #         Scale the loss and backpropagate\n",
    "#             scaler.scale(loss.mean()).backward()\n",
    "#         norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         lr = get_lr(i + clips) \n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group[\"lr\"] = lr \n",
    "            \n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         #         opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "\n",
    "#         # ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n",
    "#         # If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n",
    "#         # otherwise, optimizer.step() is skipped.\n",
    "#         #         scaler.step(optimizer)\n",
    "\n",
    "#         # Updates the scale for next iteration.\n",
    "#         #         scaler.update()\n",
    "\n",
    "#         torch.cuda.synchronize()\n",
    "#         t1 = time.time()\n",
    "#         dt = (t1 - t0) * 1000\n",
    "#         tokens_per_sec = (B * T * grad_accum_steps / dt) * 1000\n",
    "#         print(\n",
    "#             f\"step {i}, loss: {loss.mean()} , learnign rate : {lr} ,  norm : {norm} grads , dt : {dt} ms , tokens/sec : {tokens_per_sec}\"\n",
    "#         )\n",
    "#         if i >= 50:\n",
    "#             break\n",
    "#     #     print(epoch_size)\n",
    "#     #     epoch_size = epoch_size + 1\n",
    "#     # print(epoch_size)\n",
    "#     break\n",
    "\n",
    "# # import sys\n",
    "\n",
    "# # sys.exit(\n",
    "# #     0\n",
    "# # )  # ------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# # model = GPT.from_pretrained(\"gpt2\")\n",
    "# # print(\"dtidtnt creashed\")\n",
    "# # num_return_sequence = 5\n",
    "# # max_length = 30\n",
    "\n",
    "# # enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# # tokens = enc.encode(\"hello , I'm a language model\")\n",
    "# # print(tokens)\n",
    "# # tokens = torch.tensor(tokens, dtype=torch.long)  # (8 ,)\n",
    "# # tokens = tokens.unsqueeze(0).repeat(num_return_sequence, 1)  # (5 , 8)\n",
    "# # x = tokens.to(device)\n",
    "\n",
    "# # torch.manual_seed(42)\n",
    "# # if device == \"cuda\":\n",
    "# #     torch.cuda.manual_seed(42)\n",
    "# # while x.size(1) < max_length:\n",
    "# #     with torch.no_grad():\n",
    "# #         logits = model(x)\n",
    "# #         logits = logits[:, -1, :]  # (B , vocab_size)\n",
    "# #         probs = F.softmax(logits, dim=-1)\n",
    "# #         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "# #         ix = torch.multinomial(topk_probs, 1)\n",
    "# #         xcol = torch.gather(topk_indices, -1, ix)\n",
    "# #         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# # for i in range(num_return_sequence):\n",
    "# #     tokens = x[i, :max_length].tolist()\n",
    "# #     decoded = enc.decode(tokens)\n",
    "# #     print(\">\", decoded)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a0cb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T14:20:22.911454Z",
     "iopub.status.busy": "2024-11-18T14:20:22.911157Z",
     "iopub.status.idle": "2024-11-18T14:20:22.931308Z",
     "shell.execute_reply": "2024-11-18T14:20:22.930422Z"
    },
    "papermill": {
     "duration": 0.034404,
     "end_time": "2024-11-18T14:20:22.933278",
     "exception": false,
     "start_time": "2024-11-18T14:20:22.898874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model working with torch.compile() -  T4 (but only on 1 GPU)\n",
    "\n",
    "# !pip install tiktoken \n",
    "# !pip install triton \n",
    "\n",
    "# from dataclasses import dataclass\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# # prefix tokens\n",
    "# import tiktoken\n",
    "# import os\n",
    "# import json\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "# # # Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "# # # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Use GPU 0\n",
    "# # print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "# # # import os\n",
    "# # # import torch\n",
    " \n",
    "# # # Clear CUDA environment setting\n",
    "# # # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# # # Reinitialize CUDA to clear prior states\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# # Attempt to do the autodetect of the device(GPU or CPU)\n",
    "\n",
    "# device = \"cpu\"\n",
    "# torch.manual_seed(1337)\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "#     torch.cuda.manual_seed(1337)\n",
    "\n",
    "# elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "\n",
    "# print(f\"using device : {device}\")\n",
    "\n",
    "\n",
    "# # print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# class CausalSelfAttention(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         assert config.n_embd % config.n_head == 0\n",
    "\n",
    "#         self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "#         self.n_head = config.n_head\n",
    "#         self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "#         self.n_embd = config.n_embd\n",
    "#         self.c_proj.GPT_SCALE = 1\n",
    "#         self.register_buffer(\n",
    "#             \"bias\",\n",
    "#             torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "#                 1, 1, config.block_size, config.block_size\n",
    "#             ),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, T, C = (\n",
    "#             x.size()\n",
    "#         )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "#         # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "#         # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "#         # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "#         qkv = self.c_attn(x)\n",
    "#         q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "#         k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # flash attention\n",
    "#         y = (\n",
    "#             y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "#         )  # re-assemble all head outputs side by side\n",
    "#         # output projection\n",
    "#         y = self.c_proj(y)\n",
    "#         return y \n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "#         self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "#         self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "#         self.c_proj.GPT_SCALE = 1\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.c_fc(x)\n",
    "#         x = self.gelu(x)\n",
    "#         x = self.c_proj(x)\n",
    "#         return x\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "#         self.attn = CausalSelfAttention(config)\n",
    "#         self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "#         self.mlp = MLP(config)\n",
    "\n",
    "#         # attention is a communication -  aggregation function , a weigtes sum function a redcue function\n",
    "#         # MLP is a individual function  - MAP function ( MAP reduce )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.ln_1(x))  # self attention layer\n",
    "#         x = x + self.mlp(self.ln_2(x))  # feed forward network\n",
    "#         return x\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class GPTConfig:\n",
    "#     block_size: int = 1024\n",
    "#     vocab_size: int = 50257  # number of tokens : 50k + special tokens\n",
    "#     n_layers: int = 12\n",
    "#     n_head: int = 12\n",
    "#     n_embd: int = 768\n",
    "\n",
    "# class GPT(nn.Module):\n",
    "#     \"\"\"\n",
    "#     GPT (Generative Pre-trained Transformer) model class.\n",
    "\n",
    "#     Args:\n",
    "#         config (GPTConfig): Configuration object containing model hyperparameters.\n",
    "\n",
    "#     Attributes:\n",
    "#         config (GPTConfig): Configuration object containing model hyperparameters.\n",
    "#         transformer (nn.ModuleDict): Dictionary containing the transformer components:\n",
    "#             - wte (nn.Embedding): Token embedding layer.\n",
    "#             - wpe (nn.Embedding): Position embedding layer.\n",
    "#             - h (nn.ModuleList): List of transformer blocks.\n",
    "#             - ln_f (nn.LayerNorm): Final layer normalization.\n",
    "#         lm_head (nn.Linear): Linear layer for output logits.\n",
    "\n",
    "#     Methods:\n",
    "#         forward(idx):\n",
    "#             Forward pass of the GPT model.\n",
    "#             Args:\n",
    "#                 idx (torch.Tensor): Input tensor of shape (B, T) where B is the batch size and T is the sequence length.\n",
    "#             Returns:\n",
    "#                 torch.Tensor: Output logits of shape (B, T, vocab_size).\n",
    "\n",
    "#         from_pretrained(cls, model_type):\n",
    "#             Load a pre-trained GPT model from Hugging Face.\n",
    "#             Args:\n",
    "#                 model_type (str): Type of the pre-trained model to load. Must be one of {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}.\n",
    "#             Returns:\n",
    "#                 GPT: An instance of the GPT model with pre-trained weights.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "\n",
    "#         self.transformer = nn.ModuleDict(\n",
    "#             dict(\n",
    "#                 wte=nn.Embedding(config.vocab_size, config.n_embd),  # token embedding\n",
    "#                 wpe=nn.Embedding(\n",
    "#                     config.block_size, config.n_embd\n",
    "#                 ),  # position embedding\n",
    "#                 h=nn.ModuleList(\n",
    "#                     [Block(config) for _ in range(config.n_layers)]\n",
    "#                 ),  # transformer block\n",
    "#                 ln_f=nn.LayerNorm(config.n_embd),  # final layer normalization\n",
    "#             )\n",
    "#         )\n",
    "#         self.lm_head = nn.Linear(\n",
    "#             config.n_embd, config.vocab_size, bias=False\n",
    "#         )  # head for output\n",
    "#         # weight sharing scheme \n",
    "#         self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "#     def _init_weight(self, module):\n",
    "#         if isinstance(module , nn.Linear):\n",
    "#             std = 0.02\n",
    "#             if hasattr(module , 'GPT_SCALE'):\n",
    "#                 std *= (2*self.config.n_layer)**-0.5\n",
    "#             torch.nn.init.normal_(module.weight , mean =0.0 , std = std)\n",
    "#             if module.bias is not None : \n",
    "#                 torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "#             elif isinstance(module , nn.Embedding):\n",
    "#                 torch.nn.init.normal_(module.weight , mean = 0.0 , std = 0.02)\n",
    "\n",
    "#     def forward(self, idx, targets=None):\n",
    "#         # idx is of the size ( B , T )\n",
    "#         B, T = idx.size()\n",
    "#         assert (\n",
    "#             T <= self.config.block_size\n",
    "#         ), f\"cannot forward the sequence length {T} , blocksize is less than the {T}\"\n",
    "\n",
    "#         pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "#         pos_emb = self.transformer.wpe(pos)\n",
    "#         tok_emb = self.transformer.wte(idx)\n",
    "#         x = (\n",
    "#             tok_emb + pos_emb\n",
    "#         )  # broadcasting is happening here as the positional emb will be same for each of the samples in the bacth\n",
    "#         for block in self.transformer.h:\n",
    "#             x = block(x)\n",
    "#         x = self.transformer.ln_f(x)\n",
    "#         logits = self.lm_head(x)  # (B , T , vocab_size)\n",
    "#         loss = None\n",
    "#         if targets is not None:\n",
    "#             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "#         return logits, loss\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, model_type):\n",
    "#         assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "#         from transformers import GPT2LMHeadModel\n",
    "\n",
    "#         print(\"Loading model from Hugging Face: %s\" % model_type)\n",
    "\n",
    "#         config_args = {\n",
    "#             \"gpt2\": dict(\n",
    "#                 n_layers=12,\n",
    "#                 n_embd=768,\n",
    "#                 n_head=12,\n",
    "#             ),\n",
    "#             \"gpt2-medium\": dict(\n",
    "#                 n_layers=24,\n",
    "#                 n_embd=1024,\n",
    "#                 n_head=16,\n",
    "#             ),\n",
    "#             \"gpt2-large\": dict(\n",
    "#                 n_layers=36,\n",
    "#                 n_embd=1280,\n",
    "#                 n_head=20,\n",
    "#             ),\n",
    "#             \"gpt2-xl\": dict(\n",
    "#                 n_layers=48,\n",
    "#                 n_embd=1600,\n",
    "#                 n_head=25,\n",
    "#             ),\n",
    "#         }[model_type]\n",
    "\n",
    "#         config_args[\"vocab_size\"] = 50257\n",
    "#         config_args[\"block_size\"] = 1024\n",
    "#         config = GPTConfig(**config_args)\n",
    "\n",
    "#         model = GPT(config)\n",
    "#         sd = model.state_dict()\n",
    "#         sd_keys = sd.keys()\n",
    "#         sd_keys = [k for k in sd_keys if not k.endswith(\"attn.bias\")]\n",
    "#         model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "#         sd_hf = model_hf.state_dict()\n",
    "#         sd_keys_hf = sd_hf.keys()\n",
    "#         sd_keys_hf = [\n",
    "#             k for k in sd_keys_hf if not k.endswith(\"attn.masked_bias\")\n",
    "#         ]  # remove the bias for attention\n",
    "#         sd_keys_hf = [\n",
    "#             k for k in sd_keys_hf if not k.endswith(\"attn.bias\")\n",
    "#         ]  # remove the bias for attention\n",
    "#         transposed = [\n",
    "#             \"attn.c_attn.weight\",\n",
    "#             \"attn.c_proj.weight\",\n",
    "#             \"mlp.c_fc.weight\",\n",
    "#             \"mlp.c_proj.weight\",\n",
    "#         ]\n",
    "#         print(len(sd_keys_hf), len(sd_keys))\n",
    "#         assert len(sd_keys_hf) == len(\n",
    "#             sd_keys\n",
    "#         ), f\"mismatched keys : {len(sd_keys_hf)  != {len(sd_keys)}}\"\n",
    "\n",
    "#         for k in sd_keys_hf:\n",
    "#             if any(k.endswith(w) for w in transposed):\n",
    "#                 # special treatment for te conv1D weights we need to transpose the matrices\n",
    "#                 assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "#                 with torch.no_grad():\n",
    "#                     sd[k].copy_(sd_hf[k].t())\n",
    "#             else:\n",
    "#                 # vanilla copy over the other parameters\n",
    "#                 assert sd_hf[k].shape == sd[k].shape\n",
    "#                 with torch.no_grad():\n",
    "#                     sd[k].copy_(sd_hf[k])\n",
    "#         return model\n",
    "\n",
    "# import time  \n",
    "\n",
    "# model = GPT(GPTConfig())\n",
    "# # model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)\n",
    "# model.to(device)\n",
    "# model = torch.compile(model)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import IterableDataset, DataLoader\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# import queue\n",
    "# import threading\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# class PrefetchingDatasetLoader(IterableDataset):\n",
    "#     def __init__(self, file_path, tokenizer, block_size, chunk_size=1024*1024):\n",
    "#         self.file_path = file_path\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.block_size = block_size\n",
    "#         self.chunk_size = chunk_size\n",
    "\n",
    "#     def _tokenize_chunk(self, chunk):\n",
    "#         tokens = self.tokenizer.encode(chunk).ids\n",
    "#         return tokens\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         buffer = \"\"\n",
    "#         tokens = []\n",
    "#         chunk_queue = queue.Queue(maxsize=2)  # Set a small buffer for prefetching\n",
    "\n",
    "#         # Start a background thread to read and tokenize chunks\n",
    "#         def background_loader():\n",
    "#             with open(self.file_path, \"r\") as f:\n",
    "#                 while True:\n",
    "#                     chunk = f.read(self.chunk_size)\n",
    "#                     if not chunk:\n",
    "#                         break\n",
    "#                     # Tokenize and enqueue the chunk\n",
    "#                     tokenized_chunk = self._tokenize_chunk(chunk)\n",
    "#                     chunk_queue.put(tokenized_chunk)\n",
    "\n",
    "#             chunk_queue.put(None)  # Signal that loading is done\n",
    "\n",
    "#         threading.Thread(target=background_loader, daemon=True).start()\n",
    "\n",
    "#         while True:\n",
    "#             # Get the next tokenized chunk\n",
    "#             tokenized_chunk = chunk_queue.get()\n",
    "#             if tokenized_chunk is None:\n",
    "#                 break  # End of file\n",
    "\n",
    "#             tokens.extend(tokenized_chunk)\n",
    "\n",
    "#             # Yield batches from tokens\n",
    "#             while len(tokens) > self.block_size:\n",
    "#                 x = torch.tensor(tokens[:self.block_size], dtype=torch.long)\n",
    "#                 y = torch.tensor(tokens[1:self.block_size + 1], dtype=torch.long)\n",
    "#                 yield x, y\n",
    "#                 tokens = tokens[self.block_size:]\n",
    "\n",
    "#         # Yield any remaining tokens\n",
    "#         if len(tokens) > self.block_size:\n",
    "#             x = torch.tensor(tokens[:self.block_size], dtype=torch.long)\n",
    "#             y = torch.tensor(tokens[1:self.block_size + 1], dtype=torch.long)\n",
    "#             yield x, y\n",
    "\n",
    "# # Configuration\n",
    "# B, T = 8, 1024  # Batch size and sequence length\n",
    "# tokenizer = ByteLevelBPETokenizer(\n",
    "#     \"/kaggle/input/tokens-bytelevel/token_AAA/vocab.json\",\n",
    "#     \"/kaggle/input/tokens-bytelevel/token_AAA/merges.txt\"\n",
    "# )\n",
    "\n",
    "# # Create the dataset and dataloader\n",
    "# dataset = PrefetchingDatasetLoader(\"/kaggle/input/python-code-regex/python_code.txt\", tokenizer, block_size=T)\n",
    "# dataloader = DataLoader(dataset, batch_size=B, num_workers=4)  # Set `num_workers` for parallel loading\n",
    "\n",
    "# # Training loop\n",
    "# # Iterate through batches\n",
    "# # for batch in dataloader:\n",
    "# #     x, y = batch\n",
    "# #     # Your training code here\n",
    "# # scaler = torch.amp.GradScaler('cuda')\n",
    "# use_amp= True\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "# epoch_size = 0\n",
    "# for epoch in range(2):  # Number of epochs\n",
    "#     for i, (x, y) in enumerate(dataloader):\n",
    "#         t0 = time.time()\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         # print(x)\n",
    "#         # print(y)\n",
    "#         # print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "#         optimizer.zero_grad()\n",
    "# #         optimizer.zero_grad()\n",
    "        \n",
    "#         # Use autocast for mixed precision\n",
    "#         with torch.autocast(device_type=device, dtype=torch.float16 , enabled=use_amp):\n",
    "#             logits, loss = model(x, y)\n",
    "        \n",
    "# #         loss.mean().backward()\n",
    "# #         optimizer.step()\n",
    "# #         Scale the loss and backpropagate\n",
    "#         scaler.scale(loss.mean()).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "# #         opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "\n",
    "#         # ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n",
    "#         # If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n",
    "#         # otherwise, optimizer.step() is skipped.\n",
    "# #         scaler.step(optimizer)\n",
    "\n",
    "#         # Updates the scale for next iteration.\n",
    "# #         scaler.update()\n",
    "        \n",
    "#         torch.cuda.synchronize()\n",
    "#         t1 = time.time()\n",
    "#         dt = (t1-t0)*1000\n",
    "#         tokens_per_sec = (B*T/dt)*1000\n",
    "#         print(f\"step {i}, loss: {loss.mean()} , dt : {dt} ms , tokens/sec : {tokens_per_sec}\")\n",
    "#         if i >= 50:\n",
    "#             break\n",
    "#     #     print(epoch_size)\n",
    "#     #     epoch_size = epoch_size + 1 \n",
    "#     # print(epoch_size)\n",
    "#     break\n",
    "    \n",
    "# # import sys\n",
    "\n",
    "# # sys.exit(\n",
    "# #     0\n",
    "# # )  # ------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# # model = GPT.from_pretrained(\"gpt2\")\n",
    "# # print(\"dtidtnt creashed\")\n",
    "# # num_return_sequence = 5\n",
    "# # max_length = 30\n",
    "\n",
    "# # enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# # tokens = enc.encode(\"hello , I'm a language model\")\n",
    "# # print(tokens)pippi\n",
    "# # tokens = torch.tensor(tokens, dtype=torch.long)  # (8 ,)\n",
    "# # tokens = tokens.unsqueeze(0).repeat(num_return_sequence, 1)  # (5 , 8)\n",
    "# # x = tokens.to(device)\n",
    "\n",
    "# # torch.manual_seed(42)\n",
    "# # if device == \"cuda\":\n",
    "# #     torch.cuda.manual_seed(42)\n",
    "# # while x.size(1) < max_length:\n",
    "# #     with torch.no_grad():\n",
    "# #         logits = model(x)\n",
    "# #         logits = logits[:, -1, :]  # (B , vocab_size)\n",
    "# #         probs = F.softmax(logits, dim=-1)\n",
    "# #         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "# #         ix = torch.multinomial(topk_probs, 1)\n",
    "# #         xcol = torch.gather(topk_indices, -1, ix)\n",
    "# #         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# # for i in range(num_return_sequence):\n",
    "# #     tokens = x[i, :max_length].tolist()\n",
    "# #     decoded = enc.decode(tokens)\n",
    "# #     print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6555dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T14:20:22.956552Z",
     "iopub.status.busy": "2024-11-18T14:20:22.956179Z",
     "iopub.status.idle": "2024-11-18T14:20:22.960476Z",
     "shell.execute_reply": "2024-11-18T14:20:22.959625Z"
    },
    "papermill": {
     "duration": 0.018429,
     "end_time": "2024-11-18T14:20:22.962458",
     "exception": false,
     "start_time": "2024-11-18T14:20:22.944029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# kagglehub.login()\n",
    "\n",
    "# # Replace with path to directory containing model files.\n",
    "# LOCAL_MODEL_DIR = 'path/to/files'\n",
    "\n",
    "# MODEL_SLUG = 'my_model' # Replace with model slug.\n",
    "\n",
    "# # Learn more about naming model variations at\n",
    "# # https://www.kaggle.com/docs/models#name-model.\n",
    "# VARIATION_SLUG = 'default' # Replace with variation slug.\n",
    "\n",
    "# kagglehub.model_upload(\n",
    "#   handle = f\"deeperisbetter/{MODEL_SLUG}/pyTorch/{VARIATION_SLUG}\",\n",
    "#   local_model_dir = LOCAL_MODEL_DIR,\n",
    "#   version_notes = 'Update 2024-11-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e547eedb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T14:20:22.985697Z",
     "iopub.status.busy": "2024-11-18T14:20:22.985415Z",
     "iopub.status.idle": "2024-11-18T14:20:22.993399Z",
     "shell.execute_reply": "2024-11-18T14:20:22.992578Z"
    },
    "papermill": {
     "duration": 0.02175,
     "end_time": "2024-11-18T14:20:22.995218",
     "exception": false,
     "start_time": "2024-11-18T14:20:22.973468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import tiktoken\n",
    "# from datasets import load_dataset\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ------------------------------------------\n",
    "# local_dir = \"edu_fineweb10B\"\n",
    "# remote_name = \"sample-10BT\"\n",
    "# shard_size = int(1e8)  # 100M tokens per shard\n",
    "# num_shards_to_download = 95 # Limit to 8 shards per run\n",
    "# offset = 0  # Adjust this value for subsequent runs\n",
    "\n",
    "# # Create the local directory if it doesn't exist yet\n",
    "# DATA_CACHE_DIR = os.path.join(os.getcwd(), local_dir)\n",
    "# os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# # Load the dataset\n",
    "# fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
    "\n",
    "# # Initialize the tokenizer\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# eot = enc._special_tokens['<|endoftext|>' ]  # End of text token\n",
    "\n",
    "# def tokenize(doc):\n",
    "#     tokens = [eot]  # The special <|endoftext|> token delimits all documents\n",
    "#     tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "#     tokens_np = np.array(tokens, dtype=np.uint16)\n",
    "#     return tokens_np\n",
    "\n",
    "# def write_datafile(filename, tokens_np):\n",
    "#     np.save(filename, tokens_np)\n",
    "\n",
    "# # Get already downloaded files to avoid duplicates\n",
    "# downloaded_files = {f for f in os.listdir(DATA_CACHE_DIR) if f.endswith(\".npy\")}\n",
    "\n",
    "# # Tokenize and save only the required shards\n",
    "# current_shard = 0\n",
    "# token_count = 0\n",
    "# progress_bar = None\n",
    "# all_tokens_np = np.empty((shard_size,), dtype=np.uint16)  # Preallocate buffer for tokens\n",
    "\n",
    "# # Process shards based on offset and limit\n",
    "# for doc_index, doc in enumerate(tqdm(fw, desc=\"Processing Documents\")):\n",
    "#     if current_shard < offset:\n",
    "#         continue  # Skip shards up to the offset\n",
    "\n",
    "#     # Tokenize the document\n",
    "#     tokens = tokenize(doc)\n",
    "\n",
    "#     # Check if we need to start a new shard\n",
    "#     if token_count + len(tokens) > shard_size:\n",
    "#         # Write the current shard to disk\n",
    "#         filename = os.path.join(DATA_CACHE_DIR, f\"shard_{current_shard:06d}.npy\")\n",
    "#         if os.path.basename(filename) in downloaded_files:\n",
    "#             print(f\"Shard {filename} already exists, skipping.\")\n",
    "#         else:\n",
    "#             write_datafile(filename, all_tokens_np[:token_count])\n",
    "#             downloaded_files.add(os.path.basename(filename))\n",
    "#             print(f\"Saved {filename}\")\n",
    "        \n",
    "#         # Reset buffer for the next shard\n",
    "#         token_count = 0\n",
    "#         current_shard += 1\n",
    "\n",
    "#         # Stop if we've processed enough shards for this run\n",
    "#         if current_shard >= offset + num_shards_to_download:\n",
    "#             print(f\"Downloaded {num_shards_to_download} shards. Exiting.\")\n",
    "#             break\n",
    "\n",
    "#     # Add tokens to the buffer\n",
    "#     all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "#     token_count += len(tokens)\n",
    "\n",
    "# # Save any remaining tokens as the last shard\n",
    "# if token_count > 0 and current_shard < offset + num_shards_to_download:\n",
    "#     filename = os.path.join(DATA_CACHE_DIR, f\"shard_{current_shard:06d}.npy\")\n",
    "#     if os.path.basename(filename) not in downloaded_files:\n",
    "#         write_datafile(filename, all_tokens_np[:token_count])\n",
    "#         print(f\"Saved final shard {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a02dc",
   "metadata": {
    "papermill": {
     "duration": 0.010537,
     "end_time": "2024-11-18T14:20:23.016674",
     "exception": false,
     "start_time": "2024-11-18T14:20:23.006137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9f692",
   "metadata": {
    "papermill": {
     "duration": 0.010605,
     "end_time": "2024-11-18T14:20:23.038042",
     "exception": false,
     "start_time": "2024-11-18T14:20:23.027437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47049785",
   "metadata": {
    "papermill": {
     "duration": 0.010971,
     "end_time": "2024-11-18T14:20:23.059726",
     "exception": false,
     "start_time": "2024-11-18T14:20:23.048755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6083173,
     "sourceId": 9902414,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6085107,
     "sourceId": 9904927,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6105264,
     "sourceId": 9931978,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167034,
     "modelInstanceId": 144475,
     "sourceId": 169815,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167038,
     "modelInstanceId": 144479,
     "sourceId": 169820,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167053,
     "modelInstanceId": 144494,
     "sourceId": 169836,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167057,
     "modelInstanceId": 144498,
     "sourceId": 169842,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167687,
     "modelInstanceId": 145128,
     "sourceId": 170574,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167690,
     "modelInstanceId": 145131,
     "sourceId": 170577,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167701,
     "modelInstanceId": 145143,
     "sourceId": 170589,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167707,
     "modelInstanceId": 145149,
     "sourceId": 170596,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167842,
     "modelInstanceId": 145285,
     "sourceId": 170742,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167848,
     "modelInstanceId": 145292,
     "sourceId": 170750,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 50.909664,
   "end_time": "2024-11-18T14:20:23.288207",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-18T14:19:32.378543",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
