{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76e6a146",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-18T12:03:15.143347Z",
     "iopub.status.busy": "2024-11-18T12:03:15.142967Z",
     "iopub.status.idle": "2024-11-18T13:34:04.504247Z",
     "shell.execute_reply": "2024-11-18T13:34:04.503025Z"
    },
    "papermill": {
     "duration": 5449.371318,
     "end_time": "2024-11-18T13:34:04.507634",
     "exception": false,
     "start_time": "2024-11-18T12:03:15.136316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\r\n",
      "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tiktoken\r\n",
      "Successfully installed tiktoken-0.8.0\r\n",
      "Collecting triton\r\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from triton) (3.15.1)\r\n",
      "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: triton\r\n",
      "Successfully installed triton-3.1.0\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n",
      "W1118 12:04:02.083000 133245810505536 torch/distributed/run.py:779] \r\n",
      "W1118 12:04:02.083000 133245810505536 torch/distributed/run.py:779] *****************************************\r\n",
      "W1118 12:04:02.083000 133245810505536 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W1118 12:04:02.083000 133245810505536 torch/distributed/run.py:779] *****************************************\r\n",
      "total desired batch size : 524288 , grad_accum_steps : 32\r\n",
      "I am GPU  0\r\n",
      "found 94 shards for split train\r\n",
      "I am GPU  1\r\n",
      "found 1 shards for split val\r\n",
      "num decayed parameters tensor : 50 , with 203554816 parametere num decayed parameters tensor : 50 , with 203554816 parametere \r\n",
      "num non decayed parameters tensor : 98 , with 161792 parametere \r\n",
      "\r\n",
      "num non decayed parameters tensor : 98 , with 161792 parametere \r\n",
      "/kaggle/input/model31/pytorch/default/1/GPT_2_build.py:580: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\r\n",
      "/kaggle/input/model31/pytorch/default/1/GPT_2_build.py:580: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\r\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\r\n",
      "validation loss : 11.0219\r\n",
      "step     0 | loss: 11.021594 | lr 4.0000e-05 | norm: 1.8432 | dt: 96888.19ms | tok/sec: 5411.27\r\n",
      "step     1 | loss: 10.603111 | lr 8.0000e-05 | norm: 2.0165 | dt: 20904.06ms | tok/sec: 25080.68\r\n",
      "step     2 | loss: 9.918270 | lr 1.2000e-04 | norm: 2.2388 | dt: 21917.33ms | tok/sec: 23921.16\r\n",
      "step     3 | loss: 9.476265 | lr 1.6000e-04 | norm: 1.9542 | dt: 23295.12ms | tok/sec: 22506.34\r\n",
      "step     4 | loss: 9.249956 | lr 2.0000e-04 | norm: 1.9863 | dt: 22544.08ms | tok/sec: 23256.13\r\n",
      "step     5 | loss: 9.010552 | lr 2.4000e-04 | norm: 1.9178 | dt: 21656.75ms | tok/sec: 24208.98\r\n",
      "step     6 | loss: 8.798540 | lr 2.8000e-04 | norm: 1.6424 | dt: 21970.88ms | tok/sec: 23862.86\r\n",
      "step     7 | loss: 8.598901 | lr 3.2000e-04 | norm: 1.6753 | dt: 22118.44ms | tok/sec: 23703.66\r\n",
      "step     8 | loss: 8.358044 | lr 3.6000e-04 | norm: 1.0781 | dt: 21596.35ms | tok/sec: 24276.70\r\n",
      "step     9 | loss: 8.166771 | lr 4.0000e-04 | norm: 1.2482 | dt: 21377.70ms | tok/sec: 24525.00\r\n",
      "step    10 | loss: 8.011292 | lr 4.4000e-04 | norm: 0.9808 | dt: 21380.61ms | tok/sec: 24521.66\r\n",
      "step    11 | loss: 7.877153 | lr 4.8000e-04 | norm: 0.8349 | dt: 21386.07ms | tok/sec: 24515.40\r\n",
      "step    12 | loss: 7.843546 | lr 5.2000e-04 | norm: 0.8601 | dt: 21243.41ms | tok/sec: 24680.03\r\n",
      "step    13 | loss: 7.813028 | lr 5.6000e-04 | norm: 0.6951 | dt: 21097.40ms | tok/sec: 24850.84\r\n",
      "step    14 | loss: 7.695218 | lr 6.0000e-04 | norm: 0.6729 | dt: 20982.85ms | tok/sec: 24986.50\r\n",
      "step    15 | loss: 7.760964 | lr 6.4000e-04 | norm: 0.8284 | dt: 20887.59ms | tok/sec: 25100.46\r\n",
      "step    16 | loss: 7.881163 | lr 6.8000e-04 | norm: 0.8644 | dt: 20837.61ms | tok/sec: 25160.65\r\n",
      "step    17 | loss: 7.866449 | lr 7.2000e-04 | norm: 0.7901 | dt: 20836.01ms | tok/sec: 25162.59\r\n",
      "step    18 | loss: 8.129325 | lr 7.6000e-04 | norm: 0.9757 | dt: 20766.20ms | tok/sec: 25247.18\r\n",
      "step    19 | loss: 8.247272 | lr 8.0000e-04 | norm: 1.0534 | dt: 20810.18ms | tok/sec: 25193.82\r\n",
      "step    20 | loss: 7.841084 | lr 8.4000e-04 | norm: 0.7116 | dt: 20812.94ms | tok/sec: 25190.49\r\n",
      "step    21 | loss: 7.836619 | lr 8.8000e-04 | norm: 0.7895 | dt: 20980.55ms | tok/sec: 24989.24\r\n",
      "step    22 | loss: 7.790775 | lr 9.2000e-04 | norm: 0.7851 | dt: 21029.48ms | tok/sec: 24931.10\r\n",
      "step    23 | loss: 7.840581 | lr 9.6000e-04 | norm: 0.7070 | dt: 21043.63ms | tok/sec: 24914.33\r\n",
      "step    24 | loss: 7.745132 | lr 1.0000e-03 | norm: 0.7348 | dt: 21072.28ms | tok/sec: 24880.46\r\n",
      "step    25 | loss: 7.730335 | lr 1.0400e-03 | norm: 0.8251 | dt: 21099.76ms | tok/sec: 24848.05\r\n",
      "step    26 | loss: 7.780827 | lr 1.0800e-03 | norm: 0.8607 | dt: 21071.74ms | tok/sec: 24881.09\r\n",
      "step    27 | loss: 7.730934 | lr 1.1200e-03 | norm: 0.8333 | dt: 21030.01ms | tok/sec: 24930.47\r\n",
      "step    28 | loss: 7.720044 | lr 1.1600e-03 | norm: 0.4832 | dt: 20913.46ms | tok/sec: 25069.40\r\n",
      "step    29 | loss: 7.677046 | lr 1.2000e-03 | norm: 0.6127 | dt: 20832.86ms | tok/sec: 25166.39\r\n",
      "step    30 | loss: 7.825236 | lr 1.2400e-03 | norm: 0.7770 | dt: 20851.28ms | tok/sec: 25144.16\r\n",
      "step    31 | loss: 7.750551 | lr 1.2800e-03 | norm: 0.5762 | dt: 20844.64ms | tok/sec: 25152.17\r\n",
      "step    32 | loss: 7.761784 | lr 1.3200e-03 | norm: 0.6020 | dt: 20838.65ms | tok/sec: 25159.41\r\n",
      "step    33 | loss: 7.757948 | lr 1.3600e-03 | norm: 0.5892 | dt: 20871.55ms | tok/sec: 25119.75\r\n",
      "step    34 | loss: 7.667770 | lr 1.4000e-03 | norm: 0.4796 | dt: 20901.85ms | tok/sec: 25083.33\r\n",
      "step    35 | loss: 7.706055 | lr 1.4400e-03 | norm: 0.6841 | dt: 20905.11ms | tok/sec: 25079.42\r\n",
      "step    36 | loss: 7.741214 | lr 1.4800e-03 | norm: 0.5319 | dt: 20958.73ms | tok/sec: 25015.26\r\n",
      "step    37 | loss: 7.674238 | lr 1.5200e-03 | norm: 0.4852 | dt: 20958.33ms | tok/sec: 25015.74\r\n",
      "step    38 | loss: 7.674156 | lr 1.5600e-03 | norm: 0.5328 | dt: 20927.84ms | tok/sec: 25052.18\r\n",
      "step    39 | loss: 7.751331 | lr 1.6000e-03 | norm: 0.4151 | dt: 20862.71ms | tok/sec: 25130.38\r\n",
      "step    40 | loss: 7.731606 | lr 1.6400e-03 | norm: 0.4507 | dt: 20831.38ms | tok/sec: 25168.19\r\n",
      "step    41 | loss: 7.683819 | lr 1.6800e-03 | norm: 0.3719 | dt: 20830.76ms | tok/sec: 25168.93\r\n",
      "step    42 | loss: 7.669553 | lr 1.7200e-03 | norm: 0.4037 | dt: 20820.70ms | tok/sec: 25181.09\r\n",
      "step    43 | loss: 7.719020 | lr 1.7600e-03 | norm: 0.4435 | dt: 20783.18ms | tok/sec: 25226.55\r\n",
      "step    44 | loss: 7.706563 | lr 1.8000e-03 | norm: 0.3564 | dt: 20851.29ms | tok/sec: 25144.16\r\n",
      "step    45 | loss: 7.736963 | lr 1.8400e-03 | norm: 0.4321 | dt: 20901.22ms | tok/sec: 25084.09\r\n",
      "step    46 | loss: 7.728260 | lr 1.8800e-03 | norm: 0.4354 | dt: 20908.47ms | tok/sec: 25075.39\r\n",
      "step    47 | loss: 7.770788 | lr 1.9200e-03 | norm: 0.3716 | dt: 20894.46ms | tok/sec: 25092.21\r\n",
      "step    48 | loss: 7.791739 | lr 1.9600e-03 | norm: 0.4958 | dt: 20904.12ms | tok/sec: 25080.61\r\n",
      "step    49 | loss: 7.728121 | lr 2.0000e-03 | norm: 0.3663 | dt: 20947.53ms | tok/sec: 25028.63\r\n",
      "step    50 | loss: 7.805831 | lr 1.1000e-03 | norm: 0.5063 | dt: 20841.68ms | tok/sec: 25155.75\r\n",
      "step    51 | loss: 7.748118 | lr 1.1000e-03 | norm: 0.4121 | dt: 20829.53ms | tok/sec: 25170.42\r\n",
      "step    52 | loss: 7.767275 | lr 1.1000e-03 | norm: 0.2838 | dt: 20832.91ms | tok/sec: 25166.33\r\n",
      "step    53 | loss: 7.724620 | lr 1.1000e-03 | norm: 0.3296 | dt: 20848.44ms | tok/sec: 25147.59\r\n",
      "step    54 | loss: 7.753115 | lr 1.1000e-03 | norm: 0.3301 | dt: 20839.76ms | tok/sec: 25158.06\r\n",
      "step    55 | loss: 7.754826 | lr 1.1000e-03 | norm: 0.3550 | dt: 20847.38ms | tok/sec: 25148.87\r\n",
      "step    56 | loss: 7.737051 | lr 1.1000e-03 | norm: 0.2682 | dt: 20852.63ms | tok/sec: 25142.53\r\n",
      "step    57 | loss: 7.758670 | lr 1.1000e-03 | norm: 0.1940 | dt: 20901.84ms | tok/sec: 25083.35\r\n",
      "step    58 | loss: 7.720413 | lr 1.1000e-03 | norm: 0.2910 | dt: 21030.25ms | tok/sec: 24930.18\r\n",
      "step    59 | loss: 7.718457 | lr 1.1000e-03 | norm: 0.3280 | dt: 21009.14ms | tok/sec: 24955.23\r\n",
      "step    60 | loss: 7.736076 | lr 1.1000e-03 | norm: 0.2357 | dt: 21074.31ms | tok/sec: 24878.06\r\n",
      "step    61 | loss: 7.767229 | lr 1.1000e-03 | norm: 0.2870 | dt: 21075.56ms | tok/sec: 24876.58\r\n",
      "step    62 | loss: 7.697978 | lr 1.1000e-03 | norm: 0.3173 | dt: 21160.22ms | tok/sec: 24777.06\r\n",
      "step    63 | loss: 7.696512 | lr 1.1000e-03 | norm: 0.2407 | dt: 21124.03ms | tok/sec: 24819.51\r\n",
      "step    64 | loss: 7.695350 | lr 1.0999e-03 | norm: 0.3336 | dt: 20970.56ms | tok/sec: 25001.15\r\n",
      "step    65 | loss: 7.743133 | lr 1.0999e-03 | norm: 0.2933 | dt: 20971.73ms | tok/sec: 24999.75\r\n",
      "step    66 | loss: 7.689181 | lr 1.0999e-03 | norm: 0.2738 | dt: 20902.13ms | tok/sec: 25083.00\r\n",
      "step    67 | loss: 7.724659 | lr 1.0999e-03 | norm: 0.2250 | dt: 20879.76ms | tok/sec: 25109.87\r\n",
      "step    68 | loss: 7.738141 | lr 1.0999e-03 | norm: 0.3458 | dt: 20933.24ms | tok/sec: 25045.71\r\n",
      "step    69 | loss: 7.656675 | lr 1.0999e-03 | norm: 0.2988 | dt: 20914.66ms | tok/sec: 25067.97\r\n",
      "step    70 | loss: 7.665093 | lr 1.0999e-03 | norm: 0.2376 | dt: 21009.57ms | tok/sec: 24954.73\r\n",
      "step    71 | loss: 7.710052 | lr 1.0999e-03 | norm: 0.2653 | dt: 20991.59ms | tok/sec: 24976.09\r\n",
      "step    72 | loss: 7.671782 | lr 1.0999e-03 | norm: 0.2704 | dt: 20984.63ms | tok/sec: 24984.38\r\n",
      "step    73 | loss: 7.674498 | lr 1.0999e-03 | norm: 0.2472 | dt: 20944.93ms | tok/sec: 25031.74\r\n",
      "step    74 | loss: 7.635480 | lr 1.0999e-03 | norm: 0.3382 | dt: 20911.33ms | tok/sec: 25071.96\r\n",
      "step    75 | loss: 7.698298 | lr 1.0998e-03 | norm: 0.3010 | dt: 20916.15ms | tok/sec: 25066.18\r\n",
      "step    76 | loss: 7.645363 | lr 1.0998e-03 | norm: 0.2889 | dt: 20939.63ms | tok/sec: 25038.07\r\n",
      "step    77 | loss: 7.680806 | lr 1.0998e-03 | norm: 0.3001 | dt: 20900.58ms | tok/sec: 25084.85\r\n",
      "step    78 | loss: 7.678819 | lr 1.0998e-03 | norm: 0.2835 | dt: 20854.30ms | tok/sec: 25140.52\r\n",
      "step    79 | loss: 7.645099 | lr 1.0998e-03 | norm: 0.2142 | dt: 20896.29ms | tok/sec: 25090.00\r\n",
      "step    80 | loss: 7.658206 | lr 1.0998e-03 | norm: 0.2846 | dt: 20941.61ms | tok/sec: 25035.71\r\n",
      "step    81 | loss: 7.679804 | lr 1.0998e-03 | norm: 0.2581 | dt: 20963.28ms | tok/sec: 25009.83\r\n",
      "step    82 | loss: 7.647563 | lr 1.0997e-03 | norm: 0.2329 | dt: 20976.84ms | tok/sec: 24993.66\r\n",
      "step    83 | loss: 7.625600 | lr 1.0997e-03 | norm: 0.2505 | dt: 20946.62ms | tok/sec: 25029.72\r\n",
      "step    84 | loss: 7.637162 | lr 1.0997e-03 | norm: 0.2717 | dt: 20897.47ms | tok/sec: 25088.59\r\n",
      "step    85 | loss: 7.625858 | lr 1.0997e-03 | norm: 0.2547 | dt: 20890.88ms | tok/sec: 25096.50\r\n",
      "step    86 | loss: 7.635651 | lr 1.0997e-03 | norm: 0.2991 | dt: 20912.30ms | tok/sec: 25070.79\r\n",
      "step    87 | loss: 7.659805 | lr 1.0997e-03 | norm: 0.4705 | dt: 21025.54ms | tok/sec: 24935.77\r\n",
      "step    88 | loss: 7.651781 | lr 1.0996e-03 | norm: 0.2346 | dt: 21006.28ms | tok/sec: 24958.63\r\n",
      "step    89 | loss: 7.706747 | lr 1.0996e-03 | norm: 0.3393 | dt: 21034.98ms | tok/sec: 24924.58\r\n",
      "step    90 | loss: 7.651102 | lr 1.0996e-03 | norm: 0.3416 | dt: 20999.38ms | tok/sec: 24966.83\r\n",
      "step    91 | loss: 7.651520 | lr 1.0996e-03 | norm: 0.2867 | dt: 21024.26ms | tok/sec: 24937.29\r\n",
      "step    92 | loss: 7.680636 | lr 1.0995e-03 | norm: 0.2840 | dt: 21017.55ms | tok/sec: 24945.25\r\n",
      "step    93 | loss: 7.742137 | lr 1.0995e-03 | norm: 0.3180 | dt: 20984.51ms | tok/sec: 24984.52\r\n",
      "step    94 | loss: 7.752035 | lr 1.0995e-03 | norm: 0.3475 | dt: 21025.41ms | tok/sec: 24935.92\r\n",
      "step    95 | loss: 7.750534 | lr 1.0995e-03 | norm: 0.2577 | dt: 21052.31ms | tok/sec: 24904.06\r\n",
      "step    96 | loss: 7.708526 | lr 1.0995e-03 | norm: 0.3244 | dt: 21044.60ms | tok/sec: 24913.18\r\n",
      "step    97 | loss: 7.689593 | lr 1.0994e-03 | norm: 0.3005 | dt: 21034.06ms | tok/sec: 24925.66\r\n",
      "step    98 | loss: 7.729053 | lr 1.0994e-03 | norm: 0.3765 | dt: 21008.13ms | tok/sec: 24956.44\r\n",
      "step    99 | loss: 7.687081 | lr 1.0994e-03 | norm: 0.3549 | dt: 21020.10ms | tok/sec: 24942.22\r\n",
      "step   100 | loss: 7.751311 | lr 1.0994e-03 | norm: 0.3477 | dt: 20917.90ms | tok/sec: 25064.08\r\n",
      "step   101 | loss: 7.692300 | lr 1.0993e-03 | norm: 0.3582 | dt: 20917.36ms | tok/sec: 25064.73\r\n",
      "step   102 | loss: 7.687820 | lr 1.0993e-03 | norm: 0.3093 | dt: 20888.29ms | tok/sec: 25099.62\r\n",
      "step   103 | loss: 7.624796 | lr 1.0993e-03 | norm: 0.3412 | dt: 20854.29ms | tok/sec: 25140.53\r\n",
      "step   104 | loss: 7.709089 | lr 1.0993e-03 | norm: 0.3913 | dt: 20871.27ms | tok/sec: 25120.08\r\n",
      "step   105 | loss: 7.737866 | lr 1.0992e-03 | norm: 0.4426 | dt: 20934.12ms | tok/sec: 25044.67\r\n",
      "step   106 | loss: 7.693417 | lr 1.0992e-03 | norm: 0.3480 | dt: 20984.69ms | tok/sec: 24984.31\r\n",
      "step   107 | loss: 7.660545 | lr 1.0992e-03 | norm: 0.3265 | dt: 21025.59ms | tok/sec: 24935.71\r\n",
      "step   108 | loss: 7.721468 | lr 1.0991e-03 | norm: 0.4594 | dt: 20976.75ms | tok/sec: 24993.76\r\n",
      "step   109 | loss: 7.747966 | lr 1.0991e-03 | norm: 0.4088 | dt: 20933.14ms | tok/sec: 25045.84\r\n",
      "step   110 | loss: 7.680887 | lr 1.0991e-03 | norm: 0.3462 | dt: 20871.35ms | tok/sec: 25119.99\r\n",
      "step   111 | loss: 7.677747 | lr 1.0991e-03 | norm: 0.2938 | dt: 20855.24ms | tok/sec: 25139.39\r\n",
      "step   112 | loss: 7.655269 | lr 1.0990e-03 | norm: 0.3563 | dt: 20817.99ms | tok/sec: 25184.37\r\n",
      "step   113 | loss: 7.695648 | lr 1.0990e-03 | norm: 0.4038 | dt: 20764.16ms | tok/sec: 25249.66\r\n",
      "step   114 | loss: 7.725327 | lr 1.0990e-03 | norm: 0.8092 | dt: 20826.13ms | tok/sec: 25174.53\r\n",
      "step   115 | loss: 7.684533 | lr 1.0989e-03 | norm: 0.5889 | dt: 20909.76ms | tok/sec: 25073.85\r\n",
      "step   116 | loss: 7.646009 | lr 1.0989e-03 | norm: 0.6995 | dt: 20863.98ms | tok/sec: 25128.86\r\n",
      "step   117 | loss: 7.580023 | lr 1.0989e-03 | norm: 0.7759 | dt: 20925.96ms | tok/sec: 25054.43\r\n",
      "step   118 | loss: 7.619487 | lr 1.0988e-03 | norm: 0.6509 | dt: 20910.14ms | tok/sec: 25073.38\r\n",
      "step   119 | loss: 7.659967 | lr 1.0988e-03 | norm: 0.4454 | dt: 20927.25ms | tok/sec: 25052.89\r\n",
      "step   120 | loss: 7.623426 | lr 1.0988e-03 | norm: 0.6176 | dt: 20874.62ms | tok/sec: 25116.05\r\n",
      "step   121 | loss: 7.618931 | lr 1.0987e-03 | norm: 0.6413 | dt: 20834.94ms | tok/sec: 25163.89\r\n",
      "step   122 | loss: 7.608280 | lr 1.0987e-03 | norm: 0.3546 | dt: 20817.61ms | tok/sec: 25184.84\r\n",
      "step   123 | loss: 7.596895 | lr 1.0986e-03 | norm: 0.5073 | dt: 20787.44ms | tok/sec: 25221.38\r\n",
      "step   124 | loss: 7.622697 | lr 1.0986e-03 | norm: 0.5258 | dt: 20763.94ms | tok/sec: 25249.93\r\n",
      "step   125 | loss: 7.618013 | lr 1.0986e-03 | norm: 0.3968 | dt: 20806.64ms | tok/sec: 25198.11\r\n",
      "step   126 | loss: 7.621380 | lr 1.0985e-03 | norm: 0.4529 | dt: 20824.04ms | tok/sec: 25177.05\r\n",
      "step   127 | loss: 7.579350 | lr 1.0985e-03 | norm: 0.4850 | dt: 20820.36ms | tok/sec: 25181.50\r\n",
      "step   128 | loss: 7.562568 | lr 1.0984e-03 | norm: 0.5025 | dt: 20919.17ms | tok/sec: 25062.56\r\n",
      "step   129 | loss: 7.542206 | lr 1.0984e-03 | norm: 0.3821 | dt: 20924.23ms | tok/sec: 25056.51\r\n",
      "step   130 | loss: 7.611520 | lr 1.0984e-03 | norm: 0.6331 | dt: 20832.44ms | tok/sec: 25166.90\r\n",
      "step   131 | loss: 7.575740 | lr 1.0983e-03 | norm: 0.6638 | dt: 20790.27ms | tok/sec: 25217.95\r\n",
      "step   132 | loss: 7.591188 | lr 1.0983e-03 | norm: 0.7432 | dt: 20753.61ms | tok/sec: 25262.50\r\n",
      "step   133 | loss: 7.524192 | lr 1.0982e-03 | norm: 1.1518 | dt: 20846.48ms | tok/sec: 25149.95\r\n",
      "step   134 | loss: 7.509982 | lr 1.0982e-03 | norm: 0.7041 | dt: 20915.31ms | tok/sec: 25067.18\r\n",
      "step   135 | loss: 7.603204 | lr 1.0982e-03 | norm: 0.7288 | dt: 20968.62ms | tok/sec: 25003.45\r\n",
      "step   136 | loss: 7.590978 | lr 1.0981e-03 | norm: 0.7175 | dt: 20969.39ms | tok/sec: 25002.54\r\n",
      "step   137 | loss: 7.609048 | lr 1.0981e-03 | norm: 0.5517 | dt: 21028.19ms | tok/sec: 24932.63\r\n",
      "step   138 | loss: 7.551132 | lr 1.0980e-03 | norm: 0.8755 | dt: 21017.69ms | tok/sec: 24945.08\r\n",
      "step   139 | loss: 7.626335 | lr 1.0980e-03 | norm: 0.7035 | dt: 20944.40ms | tok/sec: 25032.37\r\n",
      "step   140 | loss: 7.703884 | lr 1.0979e-03 | norm: 0.5631 | dt: 20909.10ms | tok/sec: 25074.64\r\n",
      "step   141 | loss: 7.611060 | lr 1.0979e-03 | norm: 0.5204 | dt: 20885.99ms | tok/sec: 25102.37\r\n",
      "step   142 | loss: 7.663641 | lr 1.0978e-03 | norm: 0.5067 | dt: 20834.51ms | tok/sec: 25164.40\r\n",
      "step   143 | loss: 7.616823 | lr 1.0978e-03 | norm: 0.5304 | dt: 20847.51ms | tok/sec: 25148.71\r\n",
      "step   144 | loss: 7.607179 | lr 1.0977e-03 | norm: 0.5328 | dt: 20836.03ms | tok/sec: 25162.57\r\n",
      "step   145 | loss: 7.622475 | lr 1.0977e-03 | norm: 0.5542 | dt: 20852.90ms | tok/sec: 25142.21\r\n",
      "step   146 | loss: 7.626332 | lr 1.0977e-03 | norm: 0.5108 | dt: 20836.36ms | tok/sec: 25162.16\r\n",
      "step   147 | loss: 7.622531 | lr 1.0976e-03 | norm: 0.4698 | dt: 20833.40ms | tok/sec: 25165.74\r\n",
      "step   148 | loss: 7.613458 | lr 1.0976e-03 | norm: 0.4057 | dt: 20887.03ms | tok/sec: 25101.13\r\n",
      "step   149 | loss: 7.614810 | lr 1.0975e-03 | norm: 0.4807 | dt: 20937.82ms | tok/sec: 25040.24\r\n",
      "step   150 | loss: 7.619579 | lr 1.0975e-03 | norm: 0.4228 | dt: 20955.96ms | tok/sec: 25018.56\r\n",
      "step   151 | loss: 7.584661 | lr 1.0974e-03 | norm: 0.4529 | dt: 21016.64ms | tok/sec: 24946.32\r\n",
      "step   152 | loss: 7.596728 | lr 1.0973e-03 | norm: 0.6615 | dt: 21032.54ms | tok/sec: 24927.47\r\n",
      "step   153 | loss: 7.668379 | lr 1.0973e-03 | norm: 0.7652 | dt: 21073.82ms | tok/sec: 24878.64\r\n",
      "step   154 | loss: 7.572380 | lr 1.0972e-03 | norm: 0.4112 | dt: 21017.55ms | tok/sec: 24945.25\r\n",
      "step   155 | loss: 7.556790 | lr 1.0972e-03 | norm: 0.6733 | dt: 20994.06ms | tok/sec: 24973.16\r\n",
      "step   156 | loss: 7.626068 | lr 1.0971e-03 | norm: 0.3729 | dt: 20963.68ms | tok/sec: 25009.35\r\n",
      "step   157 | loss: 7.656085 | lr 1.0971e-03 | norm: 0.6428 | dt: 20930.65ms | tok/sec: 25048.82\r\n",
      "step   158 | loss: 7.633607 | lr 1.0970e-03 | norm: 0.5397 | dt: 20971.93ms | tok/sec: 24999.51\r\n",
      "step   159 | loss: 7.578245 | lr 1.0970e-03 | norm: 0.4045 | dt: 20931.88ms | tok/sec: 25047.35\r\n",
      "step   160 | loss: 7.554625 | lr 1.0969e-03 | norm: 0.4424 | dt: 20941.86ms | tok/sec: 25035.40\r\n",
      "step   161 | loss: 7.600416 | lr 1.0969e-03 | norm: 0.5008 | dt: 20964.37ms | tok/sec: 25008.52\r\n",
      "step   162 | loss: 7.520097 | lr 1.0968e-03 | norm: 0.4123 | dt: 21038.16ms | tok/sec: 24920.81\r\n",
      "step   163 | loss: 7.504493 | lr 1.0967e-03 | norm: 0.7265 | dt: 21084.17ms | tok/sec: 24866.43\r\n",
      "step   164 | loss: 7.542796 | lr 1.0967e-03 | norm: 0.5277 | dt: 21115.65ms | tok/sec: 24829.36\r\n",
      "step   165 | loss: 7.573052 | lr 1.0966e-03 | norm: 0.6555 | dt: 21145.56ms | tok/sec: 24794.24\r\n",
      "step   166 | loss: 7.462976 | lr 1.0966e-03 | norm: 0.5862 | dt: 21149.84ms | tok/sec: 24789.22\r\n",
      "step   167 | loss: 7.508318 | lr 1.0965e-03 | norm: 0.5340 | dt: 21129.93ms | tok/sec: 24812.58\r\n",
      "step   168 | loss: 7.484244 | lr 1.0965e-03 | norm: 0.5290 | dt: 21084.99ms | tok/sec: 24865.46\r\n",
      "step   169 | loss: 7.534456 | lr 1.0964e-03 | norm: 0.6038 | dt: 21119.87ms | tok/sec: 24824.40\r\n",
      "step   170 | loss: 7.492309 | lr 1.0963e-03 | norm: 0.3169 | dt: 21100.31ms | tok/sec: 24847.41\r\n",
      "step   171 | loss: 7.515570 | lr 1.0963e-03 | norm: 0.6301 | dt: 21161.01ms | tok/sec: 24776.13\r\n",
      "step   172 | loss: 7.514976 | lr 1.0962e-03 | norm: 0.8183 | dt: 21216.97ms | tok/sec: 24710.79\r\n",
      "step   173 | loss: 7.447922 | lr 1.0961e-03 | norm: 0.4307 | dt: 21277.15ms | tok/sec: 24640.89\r\n",
      "step   174 | loss: 7.475446 | lr 1.0961e-03 | norm: 1.3720 | dt: 21331.83ms | tok/sec: 24577.73\r\n",
      "step   175 | loss: 7.513222 | lr 1.0960e-03 | norm: 0.7818 | dt: 21436.70ms | tok/sec: 24457.49\r\n",
      "step   176 | loss: 7.460542 | lr 1.0960e-03 | norm: 0.7098 | dt: 21491.64ms | tok/sec: 24394.98\r\n",
      "step   177 | loss: 7.480714 | lr 1.0959e-03 | norm: 0.4089 | dt: 21475.83ms | tok/sec: 24412.94\r\n",
      "step   178 | loss: 7.420462 | lr 1.0958e-03 | norm: 0.6780 | dt: 21481.34ms | tok/sec: 24406.67\r\n",
      "step   179 | loss: 7.396295 | lr 1.0958e-03 | norm: 0.3851 | dt: 21472.49ms | tok/sec: 24416.73\r\n",
      "step   180 | loss: 7.388687 | lr 1.0957e-03 | norm: 0.6902 | dt: 21475.69ms | tok/sec: 24413.10\r\n",
      "step   181 | loss: 7.414321 | lr 1.0956e-03 | norm: 0.3477 | dt: 21460.44ms | tok/sec: 24430.44\r\n",
      "step   182 | loss: 7.403609 | lr 1.0956e-03 | norm: 0.8912 | dt: 21476.60ms | tok/sec: 24412.05\r\n",
      "step   183 | loss: 7.395911 | lr 1.0955e-03 | norm: 0.4518 | dt: 21458.12ms | tok/sec: 24433.08\r\n",
      "step   184 | loss: 7.415240 | lr 1.0954e-03 | norm: 0.7260 | dt: 21391.75ms | tok/sec: 24508.88\r\n",
      "step   185 | loss: 7.434831 | lr 1.0954e-03 | norm: 0.8297 | dt: 21434.75ms | tok/sec: 24459.72\r\n",
      "step   186 | loss: 7.448816 | lr 1.0953e-03 | norm: 0.6629 | dt: 21467.59ms | tok/sec: 24422.30\r\n",
      "step   187 | loss: 7.487355 | lr 1.0952e-03 | norm: 0.4369 | dt: 21487.50ms | tok/sec: 24399.68\r\n",
      "step   188 | loss: 7.449949 | lr 1.0951e-03 | norm: 0.4957 | dt: 21556.92ms | tok/sec: 24321.10\r\n",
      "step   189 | loss: 7.412823 | lr 1.0951e-03 | norm: 0.3833 | dt: 21522.94ms | tok/sec: 24359.50\r\n",
      "step   190 | loss: 7.441932 | lr 1.0950e-03 | norm: 0.7059 | dt: 22779.81ms | tok/sec: 23015.47\r\n",
      "step   191 | loss: 7.400358 | lr 1.0949e-03 | norm: 0.7119 | dt: 21480.77ms | tok/sec: 24407.32\r\n",
      "step   192 | loss: 7.409702 | lr 1.0949e-03 | norm: 1.1796 | dt: 21472.69ms | tok/sec: 24416.51\r\n",
      "step   193 | loss: 7.460473 | lr 1.0948e-03 | norm: 0.8778 | dt: 21469.41ms | tok/sec: 24420.23\r\n",
      "step   194 | loss: 7.348772 | lr 1.0947e-03 | norm: 0.4347 | dt: 21464.51ms | tok/sec: 24425.81\r\n",
      "step   195 | loss: 7.369246 | lr 1.0946e-03 | norm: 0.9810 | dt: 21460.57ms | tok/sec: 24430.30\r\n",
      "step   196 | loss: 7.390236 | lr 1.0946e-03 | norm: 1.1747 | dt: 21417.77ms | tok/sec: 24479.12\r\n",
      "step   197 | loss: 7.395012 | lr 1.0945e-03 | norm: 0.5870 | dt: 21427.58ms | tok/sec: 24467.91\r\n",
      "step   198 | loss: 7.458501 | lr 1.0944e-03 | norm: 1.8673 | dt: 21399.25ms | tok/sec: 24500.30\r\n",
      "step   199 | loss: 7.390352 | lr 1.0943e-03 | norm: 0.8100 | dt: 21380.45ms | tok/sec: 24521.85\r\n",
      "step   200 | loss: 7.408429 | lr 1.0943e-03 | norm: 1.6221 | dt: 21344.14ms | tok/sec: 24563.55\r\n",
      "step   201 | loss: 7.367626 | lr 1.0942e-03 | norm: 1.2974 | dt: 21285.94ms | tok/sec: 24630.72\r\n",
      "step   202 | loss: 7.376186 | lr 1.0941e-03 | norm: 1.0532 | dt: 21320.62ms | tok/sec: 24590.65\r\n",
      "step   203 | loss: 7.385562 | lr 1.0940e-03 | norm: 1.0907 | dt: 21374.93ms | tok/sec: 24528.17\r\n",
      "step   204 | loss: 7.358748 | lr 1.0940e-03 | norm: 0.7366 | dt: 21481.07ms | tok/sec: 24406.98\r\n",
      "step   205 | loss: 7.433958 | lr 1.0939e-03 | norm: 0.8338 | dt: 21503.07ms | tok/sec: 24382.00\r\n",
      "step   206 | loss: 7.426785 | lr 1.0938e-03 | norm: 0.7150 | dt: 21535.27ms | tok/sec: 24345.55\r\n",
      "step   207 | loss: 7.360828 | lr 1.0937e-03 | norm: 0.6914 | dt: 21554.62ms | tok/sec: 24323.69\r\n",
      "step   208 | loss: 7.324948 | lr 1.0936e-03 | norm: 0.9295 | dt: 21566.62ms | tok/sec: 24310.16\r\n",
      "step   209 | loss: 7.283439 | lr 1.0936e-03 | norm: 0.5241 | dt: 21557.22ms | tok/sec: 24320.76\r\n",
      "step   210 | loss: 7.312682 | lr 1.0935e-03 | norm: 0.8061 | dt: 21518.60ms | tok/sec: 24364.41\r\n",
      "step   211 | loss: 7.334882 | lr 1.0934e-03 | norm: 0.4102 | dt: 21478.83ms | tok/sec: 24409.52\r\n",
      "step   212 | loss: 7.386832 | lr 1.0933e-03 | norm: 0.7647 | dt: 21488.32ms | tok/sec: 24398.74\r\n",
      "step   213 | loss: 7.351617 | lr 1.0932e-03 | norm: 0.4199 | dt: 21483.44ms | tok/sec: 24404.28\r\n",
      "step   214 | loss: 7.344411 | lr 1.0932e-03 | norm: 0.5668 | dt: 21474.26ms | tok/sec: 24414.72\r\n",
      "step   215 | loss: 7.368340 | lr 1.0931e-03 | norm: 0.3612 | dt: 21496.35ms | tok/sec: 24389.63\r\n",
      "step   216 | loss: 7.306687 | lr 1.0930e-03 | norm: 0.4965 | dt: 21508.96ms | tok/sec: 24375.33\r\n",
      "step   217 | loss: 7.283916 | lr 1.0929e-03 | norm: 0.4466 | dt: 21558.53ms | tok/sec: 24319.28\r\n",
      "step   218 | loss: 7.290263 | lr 1.0928e-03 | norm: 0.6159 | dt: 21586.57ms | tok/sec: 24287.70\r\n",
      "step   219 | loss: 7.282586 | lr 1.0927e-03 | norm: 0.6535 | dt: 21573.35ms | tok/sec: 24302.57\r\n",
      "step   220 | loss: 7.259594 | lr 1.0926e-03 | norm: 0.5633 | dt: 21576.05ms | tok/sec: 24299.54\r\n",
      "step   221 | loss: 7.257287 | lr 1.0926e-03 | norm: 0.4980 | dt: 21582.88ms | tok/sec: 24291.85\r\n",
      "step   222 | loss: 7.299245 | lr 1.0925e-03 | norm: 0.4206 | dt: 21525.14ms | tok/sec: 24357.00\r\n",
      "step   223 | loss: 7.237997 | lr 1.0924e-03 | norm: 0.4103 | dt: 21534.61ms | tok/sec: 24346.30\r\n",
      "step   224 | loss: 7.237585 | lr 1.0923e-03 | norm: 0.6249 | dt: 21521.77ms | tok/sec: 24360.82\r\n",
      "step   225 | loss: 7.227921 | lr 1.0922e-03 | norm: 1.0393 | dt: 21489.17ms | tok/sec: 24397.78\r\n",
      "step   226 | loss: 7.288154 | lr 1.0921e-03 | norm: 3.1433 | dt: 21533.12ms | tok/sec: 24347.99\r\n",
      "step   227 | loss: 7.315540 | lr 1.0920e-03 | norm: 1.0134 | dt: 21538.88ms | tok/sec: 24341.47\r\n",
      "step   228 | loss: 7.349939 | lr 1.0919e-03 | norm: 2.7348 | dt: 21647.70ms | tok/sec: 24219.11\r\n",
      "step   229 | loss: 7.332049 | lr 1.0918e-03 | norm: 2.2954 | dt: 21716.90ms | tok/sec: 24141.93\r\n",
      "step   230 | loss: 7.278303 | lr 1.0918e-03 | norm: 0.6435 | dt: 21763.36ms | tok/sec: 24090.40\r\n",
      "step   231 | loss: 7.273050 | lr 1.0917e-03 | norm: 2.7420 | dt: 21737.37ms | tok/sec: 24119.20\r\n",
      "step   232 | loss: 7.399315 | lr 1.0916e-03 | norm: 1.0359 | dt: 21734.85ms | tok/sec: 24122.00\r\n",
      "step   233 | loss: 7.386005 | lr 1.0915e-03 | norm: 1.7968 | dt: 21635.99ms | tok/sec: 24232.21\r\n",
      "step   234 | loss: 7.387968 | lr 1.0914e-03 | norm: 1.6738 | dt: 21636.51ms | tok/sec: 24231.64\r\n",
      "step   235 | loss: 7.368963 | lr 1.0913e-03 | norm: 0.8405 | dt: 21638.06ms | tok/sec: 24229.90\r\n",
      "step   236 | loss: 7.374331 | lr 1.0912e-03 | norm: 1.1848 | dt: 21645.87ms | tok/sec: 24221.15\r\n",
      "step   237 | loss: 7.356257 | lr 1.0911e-03 | norm: 0.7854 | dt: 21635.80ms | tok/sec: 24232.43\r\n",
      "step   238 | loss: 7.306308 | lr 1.0910e-03 | norm: 0.7776 | dt: 21670.63ms | tok/sec: 24193.49\r\n",
      "step   239 | loss: 7.308734 | lr 1.0909e-03 | norm: 0.5654 | dt: 21576.02ms | tok/sec: 24299.57\r\n",
      "step   240 | loss: 7.338054 | lr 1.0908e-03 | norm: 0.9764 | dt: 21678.33ms | tok/sec: 24184.89\r\n",
      "step   241 | loss: 7.324027 | lr 1.0907e-03 | norm: 0.6369 | dt: 21596.50ms | tok/sec: 24276.53\r\n",
      "step   242 | loss: 7.306422 | lr 1.0906e-03 | norm: 1.0372 | dt: 21571.14ms | tok/sec: 24305.07\r\n",
      "step   243 | loss: 7.325035 | lr 1.0905e-03 | norm: 0.9133 | dt: 21576.83ms | tok/sec: 24298.66\r\n",
      "step   244 | loss: 7.259144 | lr 1.0904e-03 | norm: 0.6493 | dt: 21615.11ms | tok/sec: 24255.62\r\n",
      "step   245 | loss: 7.234843 | lr 1.0903e-03 | norm: 0.7237 | dt: 21664.69ms | tok/sec: 24200.12\r\n",
      "step   246 | loss: 7.242956 | lr 1.0902e-03 | norm: 0.5762 | dt: 21694.30ms | tok/sec: 24167.09\r\n",
      "step   247 | loss: 7.302084 | lr 1.0901e-03 | norm: 0.7598 | dt: 21747.95ms | tok/sec: 24107.47\r\n",
      "step   248 | loss: 7.283608 | lr 1.0900e-03 | norm: 0.6103 | dt: 21698.37ms | tok/sec: 24162.55\r\n",
      "step   249 | loss: 7.226397 | lr 1.0899e-03 | norm: 0.4155 | dt: 21710.78ms | tok/sec: 24148.74\r\n",
      "[rank1]: Traceback (most recent call last):\r\n",
      "[rank1]:   File \"/kaggle/input/model31/pytorch/default/1/GPT_2_build.py\", line 677, in <module>\r\n",
      "[rank1]:     logits, loss = model(xgen)  # (B, T, vocab_size)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n",
      "[rank1]:     return forward_call(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\r\n",
      "[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\r\n",
      "[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n",
      "[rank1]:     return forward_call(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 433, in _fn\r\n",
      "[rank1]:     return fn(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n",
      "[rank1]:     return self._call_impl(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n",
      "[rank1]:     return forward_call(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1110, in __call__\r\n",
      "[rank1]:     return hijacked_callback(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\r\n",
      "[rank1]:     result = self._inner_convert(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\r\n",
      "[rank1]:     return _compile(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\r\n",
      "[rank1]:     return StrobelightCompileTimeProfiler.profile_compile_time(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\r\n",
      "[rank1]:     return func(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n",
      "[rank1]:     return func(*args, **kwds)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\r\n",
      "[rank1]:     guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\r\n",
      "[rank1]:     r = func(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\r\n",
      "[rank1]:     out_code = transform_code_object(code, transform)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\r\n",
      "[rank1]:     transformations(instructions, code_options)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\r\n",
      "[rank1]:     return fn(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\r\n",
      "[rank1]:     tracer.run()\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\r\n",
      "[rank1]:     super().run()\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\r\n",
      "[rank1]:     while self.step():\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\r\n",
      "[rank1]:     self.dispatch_table[inst.opcode](self, inst)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\r\n",
      "[rank1]:     self._return(inst)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\r\n",
      "[rank1]:     self.output.compile_subgraph(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1123, in compile_subgraph\r\n",
      "[rank1]:     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n",
      "[rank1]:     return func(*args, **kwds)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\r\n",
      "[rank1]:     compiled_fn = self.call_user_compiler(gm)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\r\n",
      "[rank1]:     r = func(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\r\n",
      "[rank1]:     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\r\n",
      "[rank1]:     compiled_fn = compiler_fn(gm, self.example_inputs())\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/distributed.py\", line 626, in compile_fn\r\n",
      "[rank1]:     submod_compiler.run(*example_inputs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/fx/interpreter.py\", line 146, in run\r\n",
      "[rank1]:     self.env[node] = self.run_node(node)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/distributed.py\", line 348, in run_node\r\n",
      "[rank1]:     compiled_submod_real = self.compile_submod(real_mod, new_args, kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/distributed.py\", line 263, in compile_submod\r\n",
      "[rank1]:     self.compiler(input_mod, args),\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\r\n",
      "[rank1]:     compiled_gm = compiler_fn(gm, example_inputs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\r\n",
      "[rank1]:     return compile_fx(model_, inputs_, config_patches=self.config)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n",
      "[rank1]:     return func(*args, **kwds)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\r\n",
      "[rank1]:     return aot_autograd(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\r\n",
      "[rank1]:     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\r\n",
      "[rank1]:     compiled_fn, _ = create_aot_dispatcher_function(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\r\n",
      "[rank1]:     r = func(*args, **kwargs)\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\r\n",
      "[rank1]:     compiled_fn, fw_metadata = compiler_fn(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 138, in aot_dispatch_base\r\n",
      "[rank1]:     ) = fakified_out_wrapper.pre_compile(\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 494, in pre_compile\r\n",
      "[rank1]:     self.out_metas = [\r\n",
      "[rank1]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 495, in <listcomp>\r\n",
      "[rank1]:     n.meta[\"val\"] for n in (list(fw_module.graph.nodes)[-1].args[0])\r\n",
      "[rank1]: torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:\r\n",
      "[rank1]: AttributeError: 'int' object has no attribute 'meta'\r\n",
      "\r\n",
      "[rank1]: While executing %submod_4 : [num_users=4] = call_module[target=submod_4](args = (%getitem_6, %s0, %s1, %getitem_7), kwargs = {})\r\n",
      "[rank1]: Original traceback:\r\n",
      "[rank1]: None\r\n",
      "\r\n",
      "[rank1]: Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n",
      "\r\n",
      "\r\n",
      "[rank1]: You can suppress this exception and fall back to eager by setting:\r\n",
      "[rank1]:     import torch._dynamo\r\n",
      "[rank1]:     torch._dynamo.config.suppress_errors = True\r\n",
      "\r\n",
      "[rank0]: Traceback (most recent call last):\r\n",
      "[rank0]:   File \"/kaggle/input/model31/pytorch/default/1/GPT_2_build.py\", line 677, in <module>\r\n",
      "[rank0]:     logits, loss = model(xgen)  # (B, T, vocab_size)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n",
      "[rank0]:     return forward_call(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\r\n",
      "[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\r\n",
      "[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n",
      "[rank0]:     return forward_call(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 433, in _fn\r\n",
      "[rank0]:     return fn(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n",
      "[rank0]:     return self._call_impl(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n",
      "[rank0]:     return forward_call(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1110, in __call__\r\n",
      "[rank0]:     return hijacked_callback(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\r\n",
      "[rank0]:     result = self._inner_convert(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\r\n",
      "[rank0]:     return _compile(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\r\n",
      "[rank0]:     return StrobelightCompileTimeProfiler.profile_compile_time(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\r\n",
      "[rank0]:     return func(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n",
      "[rank0]:     return func(*args, **kwds)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\r\n",
      "[rank0]:     guarded_code = compile_inner(code, one_graph, hooks, transform)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\r\n",
      "[rank0]:     r = func(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\r\n",
      "[rank0]:     out_code = transform_code_object(code, transform)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\r\n",
      "[rank0]:     transformations(instructions, code_options)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\r\n",
      "[rank0]:     return fn(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\r\n",
      "[rank0]:     tracer.run()\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\r\n",
      "[rank0]:     super().run()\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\r\n",
      "[rank0]:     while self.step():\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\r\n",
      "[rank0]:     self.dispatch_table[inst.opcode](self, inst)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2642, in RETURN_VALUE\r\n",
      "[rank0]:     self._return(inst)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2627, in _return\r\n",
      "[rank0]:     self.output.compile_subgraph(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1123, in compile_subgraph\r\n",
      "[rank0]:     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n",
      "[rank0]:     return func(*args, **kwds)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1318, in compile_and_call_fx_graph\r\n",
      "[rank0]:     compiled_fn = self.call_user_compiler(gm)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\r\n",
      "[rank0]:     r = func(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1409, in call_user_compiler\r\n",
      "[rank0]:     raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/output_graph.py\", line 1390, in call_user_compiler\r\n",
      "[rank0]:     compiled_fn = compiler_fn(gm, self.example_inputs())\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/distributed.py\", line 626, in compile_fn\r\n",
      "[rank0]:     submod_compiler.run(*example_inputs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/fx/interpreter.py\", line 146, in run\r\n",
      "[rank0]:     self.env[node] = self.run_node(node)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/distributed.py\", line 348, in run_node\r\n",
      "[rank0]:     compiled_submod_real = self.compile_submod(real_mod, new_args, kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/distributed.py\", line 263, in compile_submod\r\n",
      "[rank0]:     self.compiler(input_mod, args),\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\r\n",
      "[rank0]:     compiled_gm = compiler_fn(gm, example_inputs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/__init__.py\", line 1951, in __call__\r\n",
      "[rank0]:     return compile_fx(model_, inputs_, config_patches=self.config)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/contextlib.py\", line 79, in inner\r\n",
      "[rank0]:     return func(*args, **kwds)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1505, in compile_fx\r\n",
      "[rank0]:     return aot_autograd(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/backends/common.py\", line 69, in __call__\r\n",
      "[rank0]:     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 954, in aot_module_simplified\r\n",
      "[rank0]:     compiled_fn, _ = create_aot_dispatcher_function(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\r\n",
      "[rank0]:     r = func(*args, **kwargs)\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 687, in create_aot_dispatcher_function\r\n",
      "[rank0]:     compiled_fn, fw_metadata = compiler_fn(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 138, in aot_dispatch_base\r\n",
      "[rank0]:     ) = fakified_out_wrapper.pre_compile(\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 494, in pre_compile\r\n",
      "[rank0]:     self.out_metas = [\r\n",
      "[rank0]:   File \"/opt/conda/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\", line 495, in <listcomp>\r\n",
      "[rank0]:     n.meta[\"val\"] for n in (list(fw_module.graph.nodes)[-1].args[0])\r\n",
      "[rank0]: torch._dynamo.exc.BackendCompilerFailed: backend='compile_fn' raised:\r\n",
      "[rank0]: AttributeError: 'int' object has no attribute 'meta'\r\n",
      "\r\n",
      "[rank0]: While executing %submod_4 : [num_users=4] = call_module[target=submod_4](args = (%getitem_6, %s0, %s1, %getitem_7), kwargs = {})\r\n",
      "[rank0]: Original traceback:\r\n",
      "[rank0]: None\r\n",
      "\r\n",
      "[rank0]: Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\r\n",
      "\r\n",
      "\r\n",
      "[rank0]: You can suppress this exception and fall back to eager by setting:\r\n",
      "[rank0]:     import torch._dynamo\r\n",
      "[rank0]:     torch._dynamo.config.suppress_errors = True\r\n",
      "\r\n",
      "W1118 13:34:03.944000 133245810505536 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 69 closing signal SIGTERM\r\n",
      "E1118 13:34:03.976000 133245810505536 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 68) of binary: /opt/conda/bin/python3.10\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/bin/torchrun\", line 8, in <module>\r\n",
      "    sys.exit(main())\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\r\n",
      "    return f(*args, **kwargs)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 901, in main\r\n",
      "    run(args)\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in run\r\n",
      "    elastic_launch(\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 133, in __call__\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\r\n",
      "    raise ChildFailedError(\r\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n",
      "============================================================\r\n",
      "/kaggle/input/model31/pytorch/default/1/GPT_2_build.py FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "  <NO_OTHER_FAILURES>\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2024-11-18_13:34:03\r\n",
      "  host      : 6785592f2098\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : 1 (pid: 68)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "!pip install triton\n",
    "!pip install datasets\n",
    "!torchrun --standalone --nproc_per_node=2 /kaggle/input/model31/pytorch/default/1/GPT_2_build.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f0e1f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:34:04.578355Z",
     "iopub.status.busy": "2024-11-18T13:34:04.578004Z",
     "iopub.status.idle": "2024-11-18T13:34:04.589313Z",
     "shell.execute_reply": "2024-11-18T13:34:04.588574Z"
    },
    "papermill": {
     "duration": 0.045463,
     "end_time": "2024-11-18T13:34:04.591161",
     "exception": false,
     "start_time": "2024-11-18T13:34:04.545698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Downloads and evaluates HellaSwag in Python.\n",
    "# https://github.com/rowanz/hellaswag\n",
    "\n",
    "# Example HellaSwag json item:\n",
    "\n",
    "# {\"ind\": 24, \"activity_label\": \"Roof shingle removal\", \"ctx_a\": \"A man is sitting on a roof.\", \"ctx_b\": \"he\", \"ctx\": \"A man is sitting on a roof. he\", \"split\": \"val\", \"split_type\": \"indomain\", \"label\": 3, \"endings\": [\"is using wrap to wrap a pair of skis.\", \"is ripping level tiles off.\", \"is holding a rubik's cube.\", \"starts pulling up roofing on a roof.\"], \"source_id\": \"activitynet~v_-JhWjGDPHMY\"}\n",
    "\n",
    "# ind: dataset ID\n",
    "# activity_label: The ActivityNet or WikiHow label for this example\n",
    "# context: There are two formats. The full context is in ctx. When the context ends in an (incomplete) noun phrase, like for ActivityNet, this incomplete noun phrase is in ctx_b, and the context up until then is in ctx_a. This can be useful for models such as BERT that need the last sentence to be complete. However, it's never required. If ctx_b is nonempty, then ctx is the same thing as ctx_a, followed by a space, then ctx_b.\n",
    "# endings: a list of 4 endings. The correct index is given by label (0,1,2, or 3)\n",
    "# split: train, val, or test.\n",
    "# split_type: indomain if the activity label is seen during training, else zeroshot\n",
    "# source_id: Which video or WikiHow article this example came from\n",
    "\n",
    "# gpt2 (124M)\n",
    "# - eleuther harness reports acc 28.92%, acc_norm 31.14% (multiple choice style)\n",
    "# - this script: 10042 acc: 0.2859 acc_norm: 0.2955 (completion style)\n",
    "\n",
    "# gpt2-xl (1558M)\n",
    "# - eleuther harness reports acc 40.04%, acc_norm 50.89% (multiple choice style)\n",
    "# - this script: 10042 acc: 0.3842 acc_norm: 0.4893 (completion style)\n",
    "\n",
    "# The validation set of HellaSwag has a total of 10,042 examples.\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import requests\n",
    "# import tiktoken\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "# from transformers import GPT2LMHeadModel\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "# DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), \"hellaswag\")\n",
    "\n",
    "# def download_file(url: str, fname: str, chunk_size=1024):\n",
    "#     \"\"\"Helper function to download a file from a given url\"\"\"\n",
    "#     resp = requests.get(url, stream=True)\n",
    "#     total = int(resp.headers.get(\"content-length\", 0))\n",
    "#     with open(fname, \"wb\") as file, tqdm(\n",
    "#         desc=fname,\n",
    "#         total=total,\n",
    "#         unit=\"iB\",\n",
    "#         unit_scale=True,\n",
    "#         unit_divisor=1024,\n",
    "#     ) as bar:\n",
    "#         for data in resp.iter_content(chunk_size=chunk_size):\n",
    "#             size = file.write(data)\n",
    "#             bar.update(size)\n",
    "\n",
    "# hellaswags = {\n",
    "#     \"train\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_train.jsonl\",\n",
    "#     \"val\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl\",\n",
    "#     \"test\": \"https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_test.jsonl\",\n",
    "# }\n",
    "\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# def download(split):\n",
    "#     \"\"\"Downloads HellaSwag DATA_CACHE_DIR\"\"\"\n",
    "#     os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "#     data_url = hellaswags[split]\n",
    "#     data_filename = os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\")\n",
    "#     if not os.path.exists(data_filename):\n",
    "#         print(f\"Downloading {data_url} to {data_filename}...\")\n",
    "#         download_file(data_url, data_filename)\n",
    "\n",
    "# def render_example(example):\n",
    "#     \"\"\"\n",
    "#     Given the example as a dictionary, render it as three torch tensors:\n",
    "#     - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)\n",
    "#     - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)\n",
    "#     - label (the index of the correct completion, which we hope has the highest likelihood)\n",
    "#     \"\"\"\n",
    "#     ctx = example[\"ctx\"]\n",
    "#     label = example[\"label\"]\n",
    "#     endings = example[\"endings\"]\n",
    "\n",
    "#     # data needed to reproduce this eval on the C size\n",
    "#     data = {\n",
    "#         \"label\": label,\n",
    "#         \"ctx_tokens\": None,\n",
    "#         \"ending_tokens\": [],\n",
    "#     }\n",
    "\n",
    "#     # gather up all the tokens\n",
    "#     ctx_tokens = enc.encode(ctx)\n",
    "#     data[\"ctx_tokens\"] = ctx_tokens\n",
    "#     tok_rows = []\n",
    "#     mask_rows = []\n",
    "#     for end in endings:\n",
    "#         end_tokens = enc.encode(\" \" + end) # note: prepending \" \" because GPT-2 tokenizer\n",
    "#         tok_rows.append(ctx_tokens + end_tokens)\n",
    "#         mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))\n",
    "#         data[\"ending_tokens\"].append(end_tokens)\n",
    "\n",
    "#     # have to be careful during the collation because the number of tokens in each row can differ\n",
    "#     max_len = max(len(row) for row in tok_rows)\n",
    "#     tokens = torch.zeros((4, max_len), dtype=torch.long)\n",
    "#     mask = torch.zeros((4, max_len), dtype=torch.long)\n",
    "#     for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):\n",
    "#         tokens[i, :len(tok_row)] = torch.tensor(tok_row)\n",
    "#         mask[i, :len(mask_row)] = torch.tensor(mask_row)\n",
    "\n",
    "#     return data, tokens, mask, label\n",
    "\n",
    "# def iterate_examples(split):\n",
    "#     # there are 10,042 examples in total in val\n",
    "#     download(split)\n",
    "#     with open(os.path.join(DATA_CACHE_DIR, f\"hellaswag_{split}.jsonl\"), \"r\") as f:\n",
    "#         for line in f:\n",
    "#             example = json.loads(line)\n",
    "#             yield example\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def evaluate(model_type, device):\n",
    "\n",
    "#     torch.set_float32_matmul_precision('high') # use tf32\n",
    "#     model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "#     model.to(device)\n",
    "#     # model = torch.compile(model) # optionally torch compile the model\n",
    "\n",
    "#     num_correct_norm = 0\n",
    "#     num_correct = 0\n",
    "#     num_total = 0\n",
    "#     for example in iterate_examples(\"val\"):\n",
    "#         data, tokens, mask, label = render_example(example)\n",
    "#         tokens = tokens.to(device)\n",
    "#         mask = mask.to(device)\n",
    "\n",
    "#         # get the logits\n",
    "#         logits = model(tokens).logits\n",
    "#         # evaluate the autoregressive loss at all positions\n",
    "#         shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "#         shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "#         flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "#         flat_shift_tokens = shift_tokens.view(-1)\n",
    "#         shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "#         shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "#         # now get the average loss just for the completion region (where mask == 1), in each row\n",
    "#         shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n",
    "#         masked_shift_losses = shift_losses * shift_mask\n",
    "#         # sum and divide by the number of 1s in the mask\n",
    "#         sum_loss = masked_shift_losses.sum(dim=1)\n",
    "#         avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "#         # now we have a loss for each of the 4 completions\n",
    "#         # the one with the lowest loss should be the most likely\n",
    "#         pred = sum_loss.argmin().item()\n",
    "#         pred_norm = avg_loss.argmin().item()\n",
    "\n",
    "#         # accumulate stats\n",
    "#         num_total += 1\n",
    "#         num_correct += int(pred == label)\n",
    "#         num_correct_norm += int(pred_norm == label)\n",
    "#         print(f\"{num_total} acc_norm: {num_correct_norm}/{num_total}={num_correct_norm/num_total:.4f}\")\n",
    "\n",
    "#         # debug: pretty print a few examples, and the losses in each case\n",
    "#         if num_total < 10:\n",
    "#             print(\"---\")\n",
    "#             print(f\"Context:\\n {example['ctx']}\")\n",
    "#             print(f\"Endings:\")\n",
    "#             for i, end in enumerate(example[\"endings\"]):\n",
    "#                 print(f\"{i} (loss: {avg_loss[i].item():.4f}) {end}\")\n",
    "#             print(f\"predicted: {pred_norm}, actual: {label}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"-m\", \"--model_type\", type=str, default=\"gpt2\", help=\"the model type to use\")\n",
    "#     parser.add_argument(\"-d\", \"--device\", type=str, default=\"cuda\", help=\"the device to use\")\n",
    "#     args = parser.parse_args()\n",
    "#     evaluate(args.model_type, args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a4f34",
   "metadata": {
    "papermill": {
     "duration": 0.029821,
     "end_time": "2024-11-18T13:34:04.651385",
     "exception": false,
     "start_time": "2024-11-18T13:34:04.621564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95685e14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:34:04.713121Z",
     "iopub.status.busy": "2024-11-18T13:34:04.712836Z",
     "iopub.status.idle": "2024-11-18T13:34:04.737742Z",
     "shell.execute_reply": "2024-11-18T13:34:04.737099Z"
    },
    "papermill": {
     "duration": 0.058085,
     "end_time": "2024-11-18T13:34:04.739551",
     "exception": false,
     "start_time": "2024-11-18T13:34:04.681466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # scan your code and find the ugly number , we want to get the GPU freindly code\n",
    "\n",
    "# from dataclasses import dataclass\n",
    "# import inspect\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# # prefix tokens\n",
    "# import tiktoken\n",
    "# import os\n",
    "# import json\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# torch.set_float32_matmul_precision(\"high\")\n",
    "# # # Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "# # # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Use GPU 0\n",
    "# # print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "# # # import os\n",
    "# # # import torch\n",
    "\n",
    "# # # Clear CUDA environment setting\n",
    "# # # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "# # # Reinitialize CUDA to clear prior states\n",
    "# # if torch.cuda.is_available():\n",
    "# #     torch.cuda.empty_cache()\n",
    "\n",
    "# # # Attempt to do the autodetect of the device(GPU or CPU)\n",
    "\n",
    "# # device = \"cpu\"\n",
    "# # torch.manual_seed(1337)\n",
    "# # if torch.cuda.is_available():\n",
    "# #     device = \"cuda\"\n",
    "# #     torch.cuda.manual_seed(1337)\n",
    "\n",
    "# # elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "# #     device = \"mps\"\n",
    "\n",
    "# # print(f\"using device : {device}\")\n",
    "\n",
    "\n",
    "# # print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# class CausalSelfAttention(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         assert config.n_embd % config.n_head == 0\n",
    "\n",
    "#         self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "#         self.n_head = config.n_head\n",
    "#         self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "#         self.n_embd = config.n_embd\n",
    "#         self.c_proj.GPT_SCALE = 1\n",
    "#         self.register_buffer(\n",
    "#             \"bias\",\n",
    "#             torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "#                 1, 1, config.block_size, config.block_size\n",
    "#             ),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, T, C = (\n",
    "#             x.size()\n",
    "#         )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "#         # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "#         # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "#         # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "#         qkv = self.c_attn(x)\n",
    "#         q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "#         k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # flash attention\n",
    "#         y = (\n",
    "#             y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "#         )  # re-assemble all head outputs side by side\n",
    "\n",
    "#         # output projection\n",
    "#         y = self.c_proj(y)\n",
    "#         return y\n",
    "\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "#         self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "#         self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "#         self.c_proj.GPT_SCALE = 1\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.c_fc(x)\n",
    "#         x = self.gelu(x)\n",
    "#         x = self.c_proj(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "#         self.attn = CausalSelfAttention(config)\n",
    "#         self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "#         self.mlp = MLP(config)\n",
    "\n",
    "#         # attention is a communication -  aggregation function , a weigtes sum function a redcue function\n",
    "#         # MLP is a individual function  - MAP function ( MAP reduce )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.ln_1(x))  # self attention layer\n",
    "#         x = x + self.mlp(self.ln_2(x))  # feed forward network\n",
    "#         return x\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class GPTConfig:\n",
    "#     block_size: int = 1024\n",
    "#     vocab_size: int = (\n",
    "#         50304  # number of tokens : 50k + special tokens , overwrite the 50257 with this\n",
    "#     )\n",
    "#     n_layers: int = 12\n",
    "#     n_head: int = 16\n",
    "#     n_embd: int = 1024\n",
    "\n",
    "\n",
    "# class GPT(nn.Module):\n",
    "#     \"\"\"\n",
    "#     GPT (Generative Pre-trained Transformer) model class.\n",
    "\n",
    "#     Args:\n",
    "#         config (GPTConfig): Configuration object containing model hyperparameters.\n",
    "\n",
    "#     Attributes:\n",
    "#         config (GPTConfig): Configuration object containing model hyperparameters.\n",
    "#         transformer (nn.ModuleDict): Dictionary containing the transformer components:\n",
    "#             - wte (nn.Embedding): Token embedding layer.\n",
    "#             - wpe (nn.Embedding): Position embedding layer.\n",
    "#             - h (nn.ModuleList): List of transformer blocks.\n",
    "#             - ln_f (nn.LayerNorm): Final layer normalization.\n",
    "#         lm_head (nn.Linear): Linear layer for output logits.\n",
    "\n",
    "#     Methods:\n",
    "#         forward(idx):\n",
    "#             Forward pass of the GPT model.\n",
    "#             Args:\n",
    "#                 idx (torch.Tensor): Input tensor of shape (B, T) where B is the batch size and T is the sequence length.\n",
    "#             Returns:\n",
    "#                 torch.Tensor: Output logits of shape (B, T, vocab_size).\n",
    "\n",
    "#         from_pretrained(cls, model_type):\n",
    "#             Load a pre-trained GPT model from Hugging Face.\n",
    "#             Args:\n",
    "#                 model_type (str): Type of the pre-trained model to load. Must be one of {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}.\n",
    "#             Returns:\n",
    "#                 GPT: An instance of the GPT model with pre-trained weights.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "\n",
    "#         self.transformer = nn.ModuleDict(\n",
    "#             dict(\n",
    "#                 wte=nn.Embedding(config.vocab_size, config.n_embd),  # token embedding\n",
    "#                 wpe=nn.Embedding(\n",
    "#                     config.block_size, config.n_embd\n",
    "#                 ),  # position embedding\n",
    "#                 h=nn.ModuleList(\n",
    "#                     [Block(config) for _ in range(config.n_layers)]\n",
    "#                 ),  # transformer block\n",
    "#                 ln_f=nn.LayerNorm(config.n_embd),  # final layer normalization\n",
    "#             )\n",
    "#         )\n",
    "#         self.lm_head = nn.Linear(\n",
    "#             config.n_embd, config.vocab_size, bias=False\n",
    "#         )  # head for output\n",
    "#         # weight sharing scheme\n",
    "#         self.transformer.wte.weight = self.lm_head.weight\n",
    "#         self.apply(__init_weight)\n",
    "\n",
    "#     def _init_weight(self, module):\n",
    "#         if isinstance(module, nn.Linear):\n",
    "#             std = 0.02\n",
    "#             if hasattr(module, \"GPT_SCALE\"):\n",
    "#                 std *= (2 * self.config.n_layer) ** -0.5\n",
    "#             torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "#             if module.bias is not None:\n",
    "#                 torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "#             elif isinstance(module, nn.Embedding):\n",
    "#                 torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "#     def forward(self, idx, targets=None):\n",
    "#         # idx is of the size ( B , T )\n",
    "#         B, T = idx.size()\n",
    "#         assert (\n",
    "#             T <= self.config.block_size\n",
    "#         ), f\"cannot forward the sequence length {T} , blocksize is less than the {T}\"\n",
    "\n",
    "#         pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "#         pos_emb = self.transformer.wpe(pos)\n",
    "#         tok_emb = self.transformer.wte(idx)\n",
    "#         x = (\n",
    "#             tok_emb + pos_emb\n",
    "#         )  # broadcasting is happening here as the positional emb will be same for each of the samples in the bacth\n",
    "#         for block in self.transformer.h:\n",
    "#             x = block(x)\n",
    "#         x = self.transformer.ln_f(x)\n",
    "#         logits = self.lm_head(x)  # (B , T , vocab_size)\n",
    "#         loss = None\n",
    "#         if targets is not None:\n",
    "#             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "#         return logits, loss\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, model_type):\n",
    "#         assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "#         from transformers import GPT2LMHeadModel\n",
    "\n",
    "#         print(\"Loading model from Hugging Face: %s\" % model_type)\n",
    "\n",
    "#         config_args = {\n",
    "#             \"gpt2\": dict(\n",
    "#                 n_layers=12,\n",
    "#                 n_embd=768,\n",
    "#                 n_head=12,\n",
    "#             ),\n",
    "#             \"gpt2-medium\": dict(\n",
    "#                 n_layers=24,\n",
    "#                 n_embd=1024,\n",
    "#                 n_head=16,\n",
    "#             ),\n",
    "#             \"gpt2-large\": dict(\n",
    "#                 n_layers=36,\n",
    "#                 n_embd=1280,\n",
    "#                 n_head=20,\n",
    "#             ),\n",
    "#             \"gpt2-xl\": dict(\n",
    "#                 n_layers=48,\n",
    "#                 n_embd=1600,\n",
    "#                 n_head=25,\n",
    "#             ),\n",
    "#         }[model_type]\n",
    "\n",
    "#         config_args[\"vocab_size\"] = 50257\n",
    "#         config_args[\"block_size\"] = 1024\n",
    "#         config = GPTConfig(**config_args)\n",
    "\n",
    "#         model = GPT(config)\n",
    "#         sd = model.state_dict()\n",
    "#         sd_keys = sd.keys()\n",
    "#         sd_keys = [k for k in sd_keys if not k.endswith(\"attn.bias\")]\n",
    "#         model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "#         sd_hf = model_hf.state_dict()\n",
    "#         sd_keys_hf = sd_hf.keys()\n",
    "#         sd_keys_hf = [\n",
    "#             k for k in sd_keys_hf if not k.endswith(\"attn.masked_bias\")\n",
    "#         ]  # remove the bias for attention\n",
    "#         sd_keys_hf = [\n",
    "#             k for k in sd_keys_hf if not k.endswith(\"attn.bias\")\n",
    "#         ]  # remove the bias for attention\n",
    "#         transposed = [\n",
    "#             \"attn.c_attn.weight\",\n",
    "#             \"attn.c_proj.weight\",\n",
    "#             \"mlp.c_fc.weight\",\n",
    "#             \"mlp.c_proj.weight\",\n",
    "#         ]\n",
    "#         print(len(sd_keys_hf), len(sd_keys))\n",
    "#         assert len(sd_keys_hf) == len(\n",
    "#             sd_keys\n",
    "#         ), f\"mismatched keys : {len(sd_keys_hf)  != {len(sd_keys)}}\"\n",
    "\n",
    "#         for k in sd_keys_hf:\n",
    "#             if any(k.endswith(w) for w in transposed):\n",
    "#                 # special treatment for te conv1D weights we need to transpose the matrices\n",
    "#                 assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "#                 with torch.no_grad():\n",
    "#                     sd[k].copy_(sd_hf[k].t())\n",
    "#             else:\n",
    "#                 # vanilla copy over the other parameters\n",
    "#                 assert sd_hf[k].shape == sd[k].shape\n",
    "#                 with torch.no_grad():\n",
    "#                     sd[k].copy_(sd_hf[k])\n",
    "#         return model\n",
    "    \n",
    "#     def configure_optimizer(self , weight_decay, learning_rate, epsilon):\n",
    "#         # Create the optimizer\n",
    "#         param_dict = {pm : p for pm , p in self.named_parameters() } \n",
    "#         param_dict = {k: v for k, v in param_dict.items() if v.requires_grad}\n",
    "#         decay_param = [v for k, v in param_dict.items() if v.dim() >= 2]\n",
    "#         nodecay_param = [v for k, v in param_dict.items() if v.dim()< 2 ]\n",
    "#         optim_groups = [\n",
    "#             {'params': decay_param, 'weight_decay': weight_decay},\n",
    "#             {'params': nodecay_param, 'weight_decay': 0.0}\n",
    "#         ]\n",
    "#         num_decay_params =  sum(p.numel() for p in decay_param)\n",
    "#         num_nodecay_params = sum(p.numel() for p in nodecay_param)\n",
    "#         print(f\"num decayed parameters tensor : {len(decay_param)} , with {num_decay_params} parametere \" )\n",
    "#         print(f\"num non decayed parameters tensor : {len(nodecay_param)} , with {num_nodecay_params} parametere \" )\n",
    "#         fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "#         use_fused = fused_available and 'cuda' in device\n",
    "#         optimizer = torch.optim.AdamW(\n",
    "#             optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=epsilon ,fused=use_fused\n",
    "#         )\n",
    "#         return optimizer\n",
    "\n",
    "\n",
    "# from torch.distributed import init_process_group , destroy_process_group\n",
    "\n",
    "# # ddp = int(os.environ.get(\"RANK\", -1)) != 1 \n",
    "# ddp = None\n",
    "\n",
    "# if ddp : \n",
    "#     assert torch.cuda.is_available()\n",
    "#     init_process_group(backend=\"nccl\")\n",
    "#     ddp_rank = int(os.environ[\"RANK\"])\n",
    "#     ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])  \n",
    "#     device = f\"cuda:{ddp_local_rank}\"\n",
    "#     torch.cuda.set_device(device)\n",
    "#     master_process = ddp_rank == 0\n",
    "# else:\n",
    "#     ddp_rank = 0\n",
    "#     ddp_local_rank = 0  \n",
    "#     ddp_world_size = 1  \n",
    "#     master_process = True\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     print(f\"using device : {device}\")\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "#     torch.cuda.manual_seed(1337)\n",
    "\n",
    "# import time\n",
    "\n",
    "# model = GPT(GPTConfig())\n",
    "\n",
    "# optimizer = model.configure_optimizer(weight_decay=0.1, learning_rate=6e-4, epsilon=1e-8)\n",
    "# # model = torch.nn.DataParallel(model, device_ids=[0, 1]).to(device)\n",
    "# model.to(device)\n",
    "# model = torch.compile(model)\n",
    "# # optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import IterableDataset, DataLoader\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# import queue\n",
    "# import threading\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# class PrefetchingDatasetLoader(IterableDataset):\n",
    "#     def __init__(self, file_path, tokenizer, block_size, chunk_size=1024 * 1024):\n",
    "#         self.file_path = file_path\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.block_size = block_size\n",
    "#         self.chunk_size = chunk_size\n",
    "\n",
    "#     def _tokenize_chunk(self, chunk):\n",
    "#         tokens = self.tokenizer.encode(chunk).ids\n",
    "#         return tokens\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         buffer = \"\"\n",
    "#         tokens = []\n",
    "#         chunk_queue = queue.Queue(maxsize=2)  # Set a small buffer for prefetching\n",
    "\n",
    "#         # Start a background thread to read and tokenize chunks\n",
    "#         def background_loader():\n",
    "#             with open(self.file_path, \"r\") as f:\n",
    "#                 while True:\n",
    "#                     chunk = f.read(self.chunk_size)\n",
    "#                     if not chunk:\n",
    "#                         break\n",
    "#                     # Tokenize and enqueue the chunk\n",
    "#                     tokenized_chunk = self._tokenize_chunk(chunk)\n",
    "#                     chunk_queue.put(tokenized_chunk)\n",
    "\n",
    "#             chunk_queue.put(None)  # Signal that loading is done\n",
    "\n",
    "#         threading.Thread(target=background_loader, daemon=True).start()\n",
    "\n",
    "#         while True:\n",
    "#             # Get the next tokenized chunk\n",
    "#             tokenized_chunk = chunk_queue.get()\n",
    "#             if tokenized_chunk is None:\n",
    "#                 break  # End of file\n",
    "\n",
    "#             tokens.extend(tokenized_chunk)\n",
    "\n",
    "#             # Yield batches from tokens\n",
    "#             while len(tokens) > self.block_size:\n",
    "#                 x = torch.tensor(tokens[: self.block_size], dtype=torch.long)\n",
    "#                 y = torch.tensor(tokens[1 : self.block_size + 1], dtype=torch.long)\n",
    "#                 yield x, y\n",
    "#                 tokens = tokens[self.block_size :]\n",
    "\n",
    "#         # Yield any remaining tokens\n",
    "#         if len(tokens) > self.block_size:\n",
    "#             x = torch.tensor(tokens[: self.block_size], dtype=torch.long)\n",
    "#             y = torch.tensor(tokens[1 : self.block_size + 1], dtype=torch.long)\n",
    "#             yield x, y\n",
    "\n",
    "        \n",
    "# def next_batch(dataloader_iterator):\n",
    "#     \"\"\"\n",
    "#     Fetch the next batch from the dataloader iterator.\n",
    "#     Args:\n",
    "#         dataloader_iterator: Iterator of the DataLoader.\n",
    "#     Returns:\n",
    "#         Tuple (x, y) containing the input and target tensors for the batch.\n",
    "#         Returns None if the iterator is exhausted.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return next(dataloader_iterator)\n",
    "#     except StopIteration:\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "# # Configuration\n",
    "# B, T = 8, 1024  # micro Batch size and token sequence length\n",
    "\n",
    "# total_batch_size = 524288\n",
    "# assert total_batch_size % (B*T*ddp_world_size) == 0\n",
    "# grad_accum_steps = total_batch_size//(B*T*ddp_world_size)\n",
    "# if master_process:  \n",
    "#     print(f\"total desired batch size : {total_batch_size} , grad_accum_steps : {grad_accum_steps}\")\n",
    "\n",
    "# print(\"I am GPU \", ddp_rank)\n",
    "# tokenizer = ByteLevelBPETokenizer(\n",
    "#     \"/kaggle/input/tokens-bytelevel/token_AAA/vocab.json\",\n",
    "#     \"/kaggle/input/tokens-bytelevel/token_AAA/merges.txt\",\n",
    "# )\n",
    "\n",
    "# # Create the dataset and dataloader\n",
    "# dataset = PrefetchingDatasetLoader(\n",
    "#     \"/kaggle/input/python-code-regex/python_code.txt\", tokenizer, block_size=T\n",
    "# )\n",
    "# dataloader = DataLoader(\n",
    "#     dataset, batch_size=B, num_workers=4\n",
    "# )  # Set `num_workers` for parallel loading\n",
    "\n",
    "# # Training loop\n",
    "# # Iterate through batches\n",
    "# # for batch in dataloader:\n",
    "# #     x, y = batch\n",
    "# #     # Your training code here\n",
    "# # scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# max_lr = 6e-4\n",
    "# min_lr = max_lr / 10\n",
    "# warmup_steps = 20 \n",
    "# max_steps = 100\n",
    "# def get_lr(it):\n",
    "#     if it< warmup_steps:\n",
    "#         return max_lr * (it+1) / warmup_steps\n",
    "#     if it > max_steps:\n",
    "#         return min_lr\n",
    "#     decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "#     assert 0.0 <= decay_ratio <= 1.0\n",
    "#     coeff = 0.5 * (1.0 + torch.cos(torch.tensor(decay_ratio) * 3.14159))    \n",
    "#     return min_lr + 0.5 * (max_lr - min_lr) * coeff\n",
    "\n",
    "# use_amp = True\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "# loader = iter(dataloader)\n",
    "# epoch_size = 0\n",
    "# for epoch in range(2):  # Number of epochs\n",
    "#     clips = epoch*max_steps\n",
    "#     # for i, (x, y) in enumerate(dataloader):\n",
    "#     for i in range(50):\n",
    "#         optimizer.zero_grad()\n",
    "#         t0 = time.time()\n",
    "#         loss_accum = 0.0\n",
    "#         for micro_step in range(grad_accum_steps):\n",
    "#             x , y =  next_batch(loader)\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "#             # print(x)\n",
    "#             # print(y)\n",
    "#             # print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "            \n",
    "#             #         optimizer.zero_grad()\n",
    "\n",
    "#             # Use autocast for mixed precision\n",
    "#             with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "#                 logits, loss = model(x, y)\n",
    "                \n",
    "#             loss = loss / grad_accum_steps\n",
    "#             loss_accum += loss.detach()\n",
    "#             #         loss.mean().backward()\n",
    "#             #         optimizer.step()\n",
    "#             #         Scale the loss and backpropagate\n",
    "#             scaler.scale(loss.mean()).backward()\n",
    "#         norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         lr = get_lr(i + clips) \n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group[\"lr\"] = lr \n",
    "            \n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "#         #         opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "\n",
    "#         # ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n",
    "#         # If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n",
    "#         # otherwise, optimizer.step() is skipped.\n",
    "#         #         scaler.step(optimizer)\n",
    "\n",
    "#         # Updates the scale for next iteration.\n",
    "#         #         scaler.update()\n",
    "\n",
    "#         torch.cuda.synchronize()\n",
    "#         t1 = time.time()\n",
    "#         dt = (t1 - t0) * 1000\n",
    "#         tokens_per_sec = (B * T * grad_accum_steps / dt) * 1000\n",
    "#         print(\n",
    "#             f\"step {i}, loss: {loss.mean()} , learnign rate : {lr} ,  norm : {norm} grads , dt : {dt} ms , tokens/sec : {tokens_per_sec}\"\n",
    "#         )\n",
    "#         if i >= 50:\n",
    "#             break\n",
    "#     #     print(epoch_size)\n",
    "#     #     epoch_size = epoch_size + 1\n",
    "#     # print(epoch_size)\n",
    "#     break\n",
    "\n",
    "# # import sys\n",
    "\n",
    "# # sys.exit(\n",
    "# #     0\n",
    "# # )  # ------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# # model = GPT.from_pretrained(\"gpt2\")\n",
    "# # print(\"dtidtnt creashed\")\n",
    "# # num_return_sequence = 5\n",
    "# # max_length = 30\n",
    "\n",
    "# # enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# # tokens = enc.encode(\"hello , I'm a language model\")\n",
    "# # print(tokens)\n",
    "# # tokens = torch.tensor(tokens, dtype=torch.long)  # (8 ,)\n",
    "# # tokens = tokens.unsqueeze(0).repeat(num_return_sequence, 1)  # (5 , 8)\n",
    "# # x = tokens.to(device)\n",
    "\n",
    "# # torch.manual_seed(42)\n",
    "# # if device == \"cuda\":\n",
    "# #     torch.cuda.manual_seed(42)\n",
    "# # while x.size(1) < max_length:\n",
    "# #     with torch.no_grad():\n",
    "# #         logits = model(x)\n",
    "# #         logits = logits[:, -1, :]  # (B , vocab_size)\n",
    "# #         probs = F.softmax(logits, dim=-1)\n",
    "# #         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "# #         ix = torch.multinomial(topk_probs, 1)\n",
    "# #         xcol = torch.gather(topk_indices, -1, ix)\n",
    "# #         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# # for i in range(num_return_sequence):\n",
    "# #     tokens = x[i, :max_length].tolist()\n",
    "# #     decoded = enc.decode(tokens)\n",
    "# #     print(\">\", decoded)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d40ae1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:34:04.801974Z",
     "iopub.status.busy": "2024-11-18T13:34:04.801690Z",
     "iopub.status.idle": "2024-11-18T13:34:04.820691Z",
     "shell.execute_reply": "2024-11-18T13:34:04.819864Z"
    },
    "papermill": {
     "duration": 0.052923,
     "end_time": "2024-11-18T13:34:04.822688",
     "exception": false,
     "start_time": "2024-11-18T13:34:04.769765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model working with torch.compile() -  T4 (but only on 1 GPU)\n",
    "\n",
    "# !pip install tiktoken \n",
    "# !pip install triton \n",
    "\n",
    "# from dataclasses import dataclass\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# # prefix tokens\n",
    "# import tiktoken\n",
    "# import os\n",
    "# import json\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# torch.set_float32_matmul_precision('high')\n",
    "# # # Set the CUDA_VISIBLE_DEVICES environment variable\n",
    "# # # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Use GPU 0\n",
    "# # print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "# # # import os\n",
    "# # # import torch\n",
    " \n",
    "# # # Clear CUDA environment setting\n",
    "# # # os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# # # Reinitialize CUDA to clear prior states\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "# # Attempt to do the autodetect of the device(GPU or CPU)\n",
    "\n",
    "# device = \"cpu\"\n",
    "# torch.manual_seed(1337)\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda\"\n",
    "#     torch.cuda.manual_seed(1337)\n",
    "\n",
    "# elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "#     device = \"mps\"\n",
    "\n",
    "# print(f\"using device : {device}\")\n",
    "\n",
    "\n",
    "# # print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "\n",
    "# class CausalSelfAttention(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         assert config.n_embd % config.n_head == 0\n",
    "\n",
    "#         self.c_attn = nn.Linear(config.n_embd, config.n_embd * 3)\n",
    "#         self.n_head = config.n_head\n",
    "#         self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "#         self.n_embd = config.n_embd\n",
    "#         self.c_proj.GPT_SCALE = 1\n",
    "#         self.register_buffer(\n",
    "#             \"bias\",\n",
    "#             torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "#                 1, 1, config.block_size, config.block_size\n",
    "#             ),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, T, C = (\n",
    "#             x.size()\n",
    "#         )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "#         # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "#         # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "#         # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "#         qkv = self.c_attn(x)\n",
    "#         q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "#         k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
    "#             1, 2\n",
    "#         )  # (B, nh, T, hs)\n",
    "#         y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # flash attention\n",
    "#         y = (\n",
    "#             y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "#         )  # re-assemble all head outputs side by side\n",
    "#         # output projection\n",
    "#         y = self.c_proj(y)\n",
    "#         return y \n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4)\n",
    "#         self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "#         self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "#         self.c_proj.GPT_SCALE = 1\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.c_fc(x)\n",
    "#         x = self.gelu(x)\n",
    "#         x = self.c_proj(x)\n",
    "#         return x\n",
    "\n",
    "# class Block(nn.Module):\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "#         self.attn = CausalSelfAttention(config)\n",
    "#         self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "#         self.mlp = MLP(config)\n",
    "\n",
    "#         # attention is a communication -  aggregation function , a weigtes sum function a redcue function\n",
    "#         # MLP is a individual function  - MAP function ( MAP reduce )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.ln_1(x))  # self attention layer\n",
    "#         x = x + self.mlp(self.ln_2(x))  # feed forward network\n",
    "#         return x\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class GPTConfig:\n",
    "#     block_size: int = 1024\n",
    "#     vocab_size: int = 50257  # number of tokens : 50k + special tokens\n",
    "#     n_layers: int = 12\n",
    "#     n_head: int = 12\n",
    "#     n_embd: int = 768\n",
    "\n",
    "# class GPT(nn.Module):\n",
    "#     \"\"\"\n",
    "#     GPT (Generative Pre-trained Transformer) model class.\n",
    "\n",
    "#     Args:\n",
    "#         config (GPTConfig): Configuration object containing model hyperparameters.\n",
    "\n",
    "#     Attributes:\n",
    "#         config (GPTConfig): Configuration object containing model hyperparameters.\n",
    "#         transformer (nn.ModuleDict): Dictionary containing the transformer components:\n",
    "#             - wte (nn.Embedding): Token embedding layer.\n",
    "#             - wpe (nn.Embedding): Position embedding layer.\n",
    "#             - h (nn.ModuleList): List of transformer blocks.\n",
    "#             - ln_f (nn.LayerNorm): Final layer normalization.\n",
    "#         lm_head (nn.Linear): Linear layer for output logits.\n",
    "\n",
    "#     Methods:\n",
    "#         forward(idx):\n",
    "#             Forward pass of the GPT model.\n",
    "#             Args:\n",
    "#                 idx (torch.Tensor): Input tensor of shape (B, T) where B is the batch size and T is the sequence length.\n",
    "#             Returns:\n",
    "#                 torch.Tensor: Output logits of shape (B, T, vocab_size).\n",
    "\n",
    "#         from_pretrained(cls, model_type):\n",
    "#             Load a pre-trained GPT model from Hugging Face.\n",
    "#             Args:\n",
    "#                 model_type (str): Type of the pre-trained model to load. Must be one of {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}.\n",
    "#             Returns:\n",
    "#                 GPT: An instance of the GPT model with pre-trained weights.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config) -> None:\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "\n",
    "#         self.transformer = nn.ModuleDict(\n",
    "#             dict(\n",
    "#                 wte=nn.Embedding(config.vocab_size, config.n_embd),  # token embedding\n",
    "#                 wpe=nn.Embedding(\n",
    "#                     config.block_size, config.n_embd\n",
    "#                 ),  # position embedding\n",
    "#                 h=nn.ModuleList(\n",
    "#                     [Block(config) for _ in range(config.n_layers)]\n",
    "#                 ),  # transformer block\n",
    "#                 ln_f=nn.LayerNorm(config.n_embd),  # final layer normalization\n",
    "#             )\n",
    "#         )\n",
    "#         self.lm_head = nn.Linear(\n",
    "#             config.n_embd, config.vocab_size, bias=False\n",
    "#         )  # head for output\n",
    "#         # weight sharing scheme \n",
    "#         self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "#     def _init_weight(self, module):\n",
    "#         if isinstance(module , nn.Linear):\n",
    "#             std = 0.02\n",
    "#             if hasattr(module , 'GPT_SCALE'):\n",
    "#                 std *= (2*self.config.n_layer)**-0.5\n",
    "#             torch.nn.init.normal_(module.weight , mean =0.0 , std = std)\n",
    "#             if module.bias is not None : \n",
    "#                 torch.nn.init.zeros_(module.bias)\n",
    "            \n",
    "#             elif isinstance(module , nn.Embedding):\n",
    "#                 torch.nn.init.normal_(module.weight , mean = 0.0 , std = 0.02)\n",
    "\n",
    "#     def forward(self, idx, targets=None):\n",
    "#         # idx is of the size ( B , T )\n",
    "#         B, T = idx.size()\n",
    "#         assert (\n",
    "#             T <= self.config.block_size\n",
    "#         ), f\"cannot forward the sequence length {T} , blocksize is less than the {T}\"\n",
    "\n",
    "#         pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "#         pos_emb = self.transformer.wpe(pos)\n",
    "#         tok_emb = self.transformer.wte(idx)\n",
    "#         x = (\n",
    "#             tok_emb + pos_emb\n",
    "#         )  # broadcasting is happening here as the positional emb will be same for each of the samples in the bacth\n",
    "#         for block in self.transformer.h:\n",
    "#             x = block(x)\n",
    "#         x = self.transformer.ln_f(x)\n",
    "#         logits = self.lm_head(x)  # (B , T , vocab_size)\n",
    "#         loss = None\n",
    "#         if targets is not None:\n",
    "#             loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "#         return logits, loss\n",
    "\n",
    "#     @classmethod\n",
    "#     def from_pretrained(cls, model_type):\n",
    "#         assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "#         from transformers import GPT2LMHeadModel\n",
    "\n",
    "#         print(\"Loading model from Hugging Face: %s\" % model_type)\n",
    "\n",
    "#         config_args = {\n",
    "#             \"gpt2\": dict(\n",
    "#                 n_layers=12,\n",
    "#                 n_embd=768,\n",
    "#                 n_head=12,\n",
    "#             ),\n",
    "#             \"gpt2-medium\": dict(\n",
    "#                 n_layers=24,\n",
    "#                 n_embd=1024,\n",
    "#                 n_head=16,\n",
    "#             ),\n",
    "#             \"gpt2-large\": dict(\n",
    "#                 n_layers=36,\n",
    "#                 n_embd=1280,\n",
    "#                 n_head=20,\n",
    "#             ),\n",
    "#             \"gpt2-xl\": dict(\n",
    "#                 n_layers=48,\n",
    "#                 n_embd=1600,\n",
    "#                 n_head=25,\n",
    "#             ),\n",
    "#         }[model_type]\n",
    "\n",
    "#         config_args[\"vocab_size\"] = 50257\n",
    "#         config_args[\"block_size\"] = 1024\n",
    "#         config = GPTConfig(**config_args)\n",
    "\n",
    "#         model = GPT(config)\n",
    "#         sd = model.state_dict()\n",
    "#         sd_keys = sd.keys()\n",
    "#         sd_keys = [k for k in sd_keys if not k.endswith(\"attn.bias\")]\n",
    "#         model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "#         sd_hf = model_hf.state_dict()\n",
    "#         sd_keys_hf = sd_hf.keys()\n",
    "#         sd_keys_hf = [\n",
    "#             k for k in sd_keys_hf if not k.endswith(\"attn.masked_bias\")\n",
    "#         ]  # remove the bias for attention\n",
    "#         sd_keys_hf = [\n",
    "#             k for k in sd_keys_hf if not k.endswith(\"attn.bias\")\n",
    "#         ]  # remove the bias for attention\n",
    "#         transposed = [\n",
    "#             \"attn.c_attn.weight\",\n",
    "#             \"attn.c_proj.weight\",\n",
    "#             \"mlp.c_fc.weight\",\n",
    "#             \"mlp.c_proj.weight\",\n",
    "#         ]\n",
    "#         print(len(sd_keys_hf), len(sd_keys))\n",
    "#         assert len(sd_keys_hf) == len(\n",
    "#             sd_keys\n",
    "#         ), f\"mismatched keys : {len(sd_keys_hf)  != {len(sd_keys)}}\"\n",
    "\n",
    "#         for k in sd_keys_hf:\n",
    "#             if any(k.endswith(w) for w in transposed):\n",
    "#                 # special treatment for te conv1D weights we need to transpose the matrices\n",
    "#                 assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "#                 with torch.no_grad():\n",
    "#                     sd[k].copy_(sd_hf[k].t())\n",
    "#             else:\n",
    "#                 # vanilla copy over the other parameters\n",
    "#                 assert sd_hf[k].shape == sd[k].shape\n",
    "#                 with torch.no_grad():\n",
    "#                     sd[k].copy_(sd_hf[k])\n",
    "#         return model\n",
    "\n",
    "# import time  \n",
    "\n",
    "# model = GPT(GPTConfig())\n",
    "# # model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)\n",
    "# model.to(device)\n",
    "# model = torch.compile(model)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import IterableDataset, DataLoader\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "# import queue\n",
    "# import threading\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# class PrefetchingDatasetLoader(IterableDataset):\n",
    "#     def __init__(self, file_path, tokenizer, block_size, chunk_size=1024*1024):\n",
    "#         self.file_path = file_path\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.block_size = block_size\n",
    "#         self.chunk_size = chunk_size\n",
    "\n",
    "#     def _tokenize_chunk(self, chunk):\n",
    "#         tokens = self.tokenizer.encode(chunk).ids\n",
    "#         return tokens\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         buffer = \"\"\n",
    "#         tokens = []\n",
    "#         chunk_queue = queue.Queue(maxsize=2)  # Set a small buffer for prefetching\n",
    "\n",
    "#         # Start a background thread to read and tokenize chunks\n",
    "#         def background_loader():\n",
    "#             with open(self.file_path, \"r\") as f:\n",
    "#                 while True:\n",
    "#                     chunk = f.read(self.chunk_size)\n",
    "#                     if not chunk:\n",
    "#                         break\n",
    "#                     # Tokenize and enqueue the chunk\n",
    "#                     tokenized_chunk = self._tokenize_chunk(chunk)\n",
    "#                     chunk_queue.put(tokenized_chunk)\n",
    "\n",
    "#             chunk_queue.put(None)  # Signal that loading is done\n",
    "\n",
    "#         threading.Thread(target=background_loader, daemon=True).start()\n",
    "\n",
    "#         while True:\n",
    "#             # Get the next tokenized chunk\n",
    "#             tokenized_chunk = chunk_queue.get()\n",
    "#             if tokenized_chunk is None:\n",
    "#                 break  # End of file\n",
    "\n",
    "#             tokens.extend(tokenized_chunk)\n",
    "\n",
    "#             # Yield batches from tokens\n",
    "#             while len(tokens) > self.block_size:\n",
    "#                 x = torch.tensor(tokens[:self.block_size], dtype=torch.long)\n",
    "#                 y = torch.tensor(tokens[1:self.block_size + 1], dtype=torch.long)\n",
    "#                 yield x, y\n",
    "#                 tokens = tokens[self.block_size:]\n",
    "\n",
    "#         # Yield any remaining tokens\n",
    "#         if len(tokens) > self.block_size:\n",
    "#             x = torch.tensor(tokens[:self.block_size], dtype=torch.long)\n",
    "#             y = torch.tensor(tokens[1:self.block_size + 1], dtype=torch.long)\n",
    "#             yield x, y\n",
    "\n",
    "# # Configuration\n",
    "# B, T = 8, 1024  # Batch size and sequence length\n",
    "# tokenizer = ByteLevelBPETokenizer(\n",
    "#     \"/kaggle/input/tokens-bytelevel/token_AAA/vocab.json\",\n",
    "#     \"/kaggle/input/tokens-bytelevel/token_AAA/merges.txt\"\n",
    "# )\n",
    "\n",
    "# # Create the dataset and dataloader\n",
    "# dataset = PrefetchingDatasetLoader(\"/kaggle/input/python-code-regex/python_code.txt\", tokenizer, block_size=T)\n",
    "# dataloader = DataLoader(dataset, batch_size=B, num_workers=4)  # Set `num_workers` for parallel loading\n",
    "\n",
    "# # Training loop\n",
    "# # Iterate through batches\n",
    "# # for batch in dataloader:\n",
    "# #     x, y = batch\n",
    "# #     # Your training code here\n",
    "# # scaler = torch.amp.GradScaler('cuda')\n",
    "# use_amp= True\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "# epoch_size = 0\n",
    "# for epoch in range(2):  # Number of epochs\n",
    "#     for i, (x, y) in enumerate(dataloader):\n",
    "#         t0 = time.time()\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         # print(x)\n",
    "#         # print(y)\n",
    "#         # print(\"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "#         optimizer.zero_grad()\n",
    "# #         optimizer.zero_grad()\n",
    "        \n",
    "#         # Use autocast for mixed precision\n",
    "#         with torch.autocast(device_type=device, dtype=torch.float16 , enabled=use_amp):\n",
    "#             logits, loss = model(x, y)\n",
    "        \n",
    "# #         loss.mean().backward()\n",
    "# #         optimizer.step()\n",
    "# #         Scale the loss and backpropagate\n",
    "#         scaler.scale(loss.mean()).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "# #         opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "\n",
    "#         # ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n",
    "#         # If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n",
    "#         # otherwise, optimizer.step() is skipped.\n",
    "# #         scaler.step(optimizer)\n",
    "\n",
    "#         # Updates the scale for next iteration.\n",
    "# #         scaler.update()\n",
    "        \n",
    "#         torch.cuda.synchronize()\n",
    "#         t1 = time.time()\n",
    "#         dt = (t1-t0)*1000\n",
    "#         tokens_per_sec = (B*T/dt)*1000\n",
    "#         print(f\"step {i}, loss: {loss.mean()} , dt : {dt} ms , tokens/sec : {tokens_per_sec}\")\n",
    "#         if i >= 50:\n",
    "#             break\n",
    "#     #     print(epoch_size)\n",
    "#     #     epoch_size = epoch_size + 1 \n",
    "#     # print(epoch_size)\n",
    "#     break\n",
    "    \n",
    "# # import sys\n",
    "\n",
    "# # sys.exit(\n",
    "# #     0\n",
    "# # )  # ------------------------------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "# # model = GPT.from_pretrained(\"gpt2\")\n",
    "# # print(\"dtidtnt creashed\")\n",
    "# # num_return_sequence = 5\n",
    "# # max_length = 30\n",
    "\n",
    "# # enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# # tokens = enc.encode(\"hello , I'm a language model\")\n",
    "# # print(tokens)pippi\n",
    "# # tokens = torch.tensor(tokens, dtype=torch.long)  # (8 ,)\n",
    "# # tokens = tokens.unsqueeze(0).repeat(num_return_sequence, 1)  # (5 , 8)\n",
    "# # x = tokens.to(device)\n",
    "\n",
    "# # torch.manual_seed(42)\n",
    "# # if device == \"cuda\":\n",
    "# #     torch.cuda.manual_seed(42)\n",
    "# # while x.size(1) < max_length:\n",
    "# #     with torch.no_grad():\n",
    "# #         logits = model(x)\n",
    "# #         logits = logits[:, -1, :]  # (B , vocab_size)\n",
    "# #         probs = F.softmax(logits, dim=-1)\n",
    "# #         topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "# #         ix = torch.multinomial(topk_probs, 1)\n",
    "# #         xcol = torch.gather(topk_indices, -1, ix)\n",
    "# #         x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# # for i in range(num_return_sequence):\n",
    "# #     tokens = x[i, :max_length].tolist()\n",
    "# #     decoded = enc.decode(tokens)\n",
    "# #     print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f96302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:34:04.885830Z",
     "iopub.status.busy": "2024-11-18T13:34:04.885207Z",
     "iopub.status.idle": "2024-11-18T13:34:04.889295Z",
     "shell.execute_reply": "2024-11-18T13:34:04.888479Z"
    },
    "papermill": {
     "duration": 0.037698,
     "end_time": "2024-11-18T13:34:04.891068",
     "exception": false,
     "start_time": "2024-11-18T13:34:04.853370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# kagglehub.login()\n",
    "\n",
    "# # Replace with path to directory containing model files.\n",
    "# LOCAL_MODEL_DIR = 'path/to/files'\n",
    "\n",
    "# MODEL_SLUG = 'my_model' # Replace with model slug.\n",
    "\n",
    "# # Learn more about naming model variations at\n",
    "# # https://www.kaggle.com/docs/models#name-model.\n",
    "# VARIATION_SLUG = 'default' # Replace with variation slug.\n",
    "\n",
    "# kagglehub.model_upload(\n",
    "#   handle = f\"deeperisbetter/{MODEL_SLUG}/pyTorch/{VARIATION_SLUG}\",\n",
    "#   local_model_dir = LOCAL_MODEL_DIR,\n",
    "#   version_notes = 'Update 2024-11-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39edb5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T13:34:04.953047Z",
     "iopub.status.busy": "2024-11-18T13:34:04.952699Z",
     "iopub.status.idle": "2024-11-18T13:34:04.958993Z",
     "shell.execute_reply": "2024-11-18T13:34:04.958207Z"
    },
    "papermill": {
     "duration": 0.039443,
     "end_time": "2024-11-18T13:34:04.960927",
     "exception": false,
     "start_time": "2024-11-18T13:34:04.921484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import tiktoken\n",
    "# from datasets import load_dataset\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # ------------------------------------------\n",
    "# local_dir = \"edu_fineweb10B\"\n",
    "# remote_name = \"sample-10BT\"\n",
    "# shard_size = int(1e8)  # 100M tokens per shard\n",
    "# num_shards_to_download = 95 # Limit to 8 shards per run\n",
    "# offset = 0  # Adjust this value for subsequent runs\n",
    "\n",
    "# # Create the local directory if it doesn't exist yet\n",
    "# DATA_CACHE_DIR = os.path.join(os.getcwd(), local_dir)\n",
    "# os.makedirs(DATA_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# # Load the dataset\n",
    "# fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\")\n",
    "\n",
    "# # Initialize the tokenizer\n",
    "# enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# eot = enc._special_tokens['<|endoftext|>' ]  # End of text token\n",
    "\n",
    "# def tokenize(doc):\n",
    "#     tokens = [eot]  # The special <|endoftext|> token delimits all documents\n",
    "#     tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "#     tokens_np = np.array(tokens, dtype=np.uint16)\n",
    "#     return tokens_np\n",
    "\n",
    "# def write_datafile(filename, tokens_np):\n",
    "#     np.save(filename, tokens_np)\n",
    "\n",
    "# # Get already downloaded files to avoid duplicates\n",
    "# downloaded_files = {f for f in os.listdir(DATA_CACHE_DIR) if f.endswith(\".npy\")}\n",
    "\n",
    "# # Tokenize and save only the required shards\n",
    "# current_shard = 0\n",
    "# token_count = 0\n",
    "# progress_bar = None\n",
    "# all_tokens_np = np.empty((shard_size,), dtype=np.uint16)  # Preallocate buffer for tokens\n",
    "\n",
    "# # Process shards based on offset and limit\n",
    "# for doc_index, doc in enumerate(tqdm(fw, desc=\"Processing Documents\")):\n",
    "#     if current_shard < offset:\n",
    "#         continue  # Skip shards up to the offset\n",
    "\n",
    "#     # Tokenize the document\n",
    "#     tokens = tokenize(doc)\n",
    "\n",
    "#     # Check if we need to start a new shard\n",
    "#     if token_count + len(tokens) > shard_size:\n",
    "#         # Write the current shard to disk\n",
    "#         filename = os.path.join(DATA_CACHE_DIR, f\"shard_{current_shard:06d}.npy\")\n",
    "#         if os.path.basename(filename) in downloaded_files:\n",
    "#             print(f\"Shard {filename} already exists, skipping.\")\n",
    "#         else:\n",
    "#             write_datafile(filename, all_tokens_np[:token_count])\n",
    "#             downloaded_files.add(os.path.basename(filename))\n",
    "#             print(f\"Saved {filename}\")\n",
    "        \n",
    "#         # Reset buffer for the next shard\n",
    "#         token_count = 0\n",
    "#         current_shard += 1\n",
    "\n",
    "#         # Stop if we've processed enough shards for this run\n",
    "#         if current_shard >= offset + num_shards_to_download:\n",
    "#             print(f\"Downloaded {num_shards_to_download} shards. Exiting.\")\n",
    "#             break\n",
    "\n",
    "#     # Add tokens to the buffer\n",
    "#     all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
    "#     token_count += len(tokens)\n",
    "\n",
    "# # Save any remaining tokens as the last shard\n",
    "# if token_count > 0 and current_shard < offset + num_shards_to_download:\n",
    "#     filename = os.path.join(DATA_CACHE_DIR, f\"shard_{current_shard:06d}.npy\")\n",
    "#     if os.path.basename(filename) not in downloaded_files:\n",
    "#         write_datafile(filename, all_tokens_np[:token_count])\n",
    "#         print(f\"Saved final shard {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940726dc",
   "metadata": {
    "papermill": {
     "duration": 0.030067,
     "end_time": "2024-11-18T13:34:05.021721",
     "exception": false,
     "start_time": "2024-11-18T13:34:04.991654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3a204",
   "metadata": {
    "papermill": {
     "duration": 0.029879,
     "end_time": "2024-11-18T13:34:05.082161",
     "exception": false,
     "start_time": "2024-11-18T13:34:05.052282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b5070",
   "metadata": {
    "papermill": {
     "duration": 0.030205,
     "end_time": "2024-11-18T13:34:05.142458",
     "exception": false,
     "start_time": "2024-11-18T13:34:05.112253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6083173,
     "sourceId": 9902414,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6085107,
     "sourceId": 9904927,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6105264,
     "sourceId": 9931978,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167034,
     "modelInstanceId": 144475,
     "sourceId": 169815,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167038,
     "modelInstanceId": 144479,
     "sourceId": 169820,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167053,
     "modelInstanceId": 144494,
     "sourceId": 169836,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167057,
     "modelInstanceId": 144498,
     "sourceId": 169842,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167687,
     "modelInstanceId": 145128,
     "sourceId": 170574,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167690,
     "modelInstanceId": 145131,
     "sourceId": 170577,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167701,
     "modelInstanceId": 145143,
     "sourceId": 170589,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 167707,
     "modelInstanceId": 145149,
     "sourceId": 170596,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5452.7103,
   "end_time": "2024-11-18T13:34:05.490400",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-18T12:03:12.780100",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
